{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b85afebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import mne\n",
    "import xml.etree.ElementTree as ET\n",
    "from scipy.signal import butter, filtfilt, iirnotch, welch\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88668f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BASE_DIR = \"../../data/MESA\"\n",
    "EDF_DIR = os.path.join(BASE_DIR, 'edfs')\n",
    "ANNOTATION_DIR = os.path.join(BASE_DIR, 'mesa_200_annotations-events-nssr')\n",
    "OUTPUT_DIR = '../../new_processed_mesa'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85e7e926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 EDF files to process across specified directories.\n"
     ]
    }
   ],
   "source": [
    "# --- Locate all EDF files across multiple subdirectories ---\n",
    "# List of directory names containing EDFs under BASE_DIR\n",
    "edf_subdir_names = ['mesa_200_subset_edfs'] + [f'mesa_200_subset_edfs {i}' for i in range(2, 19)]\n",
    "all_edf_files = []\n",
    "for subdir_name in edf_subdir_names:\n",
    "    dir_path = os.path.join(EDF_DIR, subdir_name)\n",
    "    if os.path.isdir(dir_path):\n",
    "        edf_files_in_subdir = glob.glob(os.path.join(dir_path, '*.edf')) + \\\n",
    "                              glob.glob(os.path.join(dir_path, '*.EDF'))\n",
    "        all_edf_files.extend(edf_files_in_subdir)\n",
    "    else:\n",
    "        print(f\"Warning: Directory not found - {dir_path}\")\n",
    "\n",
    "# Remove duplicates if any file exists in multiple lists and sort\n",
    "all_edf_files = sorted(list(set(all_edf_files)))\n",
    "\n",
    "print(f\"Found {len(all_edf_files)} EDF files to process across specified directories.\")\n",
    "# --- End of EDF file location ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cc9a95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of already-processed PSG recording IDs (without .edf)\n",
    "PROCESSED_ALREADY = [\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64871385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EEG 1 is the same as EEG Fz-Cz\n",
    "# EEG 2 is the same as EEG Cz-Oz\n",
    "# EEG 3 is the same as EEG C4 M1\n",
    "# Kim's source: https://sleepdata.org/datasets/mesa/pages/equipment/montage-and-sampling-rate-information.md \n",
    "\n",
    "CHANNELS_TO_LOAD = [\"EEG1\", \"EOG-L\", \"EOG-R\"]  # Adjust based on available channels\n",
    "TARGET_SFREQ = 256.0\n",
    "LOW_FREQ = 0.5\n",
    "HIGH_FREQ = 30.0\n",
    "EPOCH_LENGTH = 30.0\n",
    "SEQ_LENGTH = 20\n",
    "SEQ_STRIDE = 10\n",
    "\n",
    "# Sleep stage mapping based on NSRR annotations\n",
    "ANNOTATION_MAP = {\n",
    "    \"0\": 0,  # Wake\n",
    "    \"1\": 1,  # N1\n",
    "    \"2\": 2,  # N2\n",
    "    \"3\": 3,  # N3\n",
    "    \"5\": 4   # REM\n",
    "}\n",
    "\n",
    "def auto_notch_filter(signal, fs, center_guess=60.0, search_range=1.0, quality=30.0):\n",
    "    freqs, psd = welch(signal, fs=fs, nperseg=2048)\n",
    "    idx = np.where((freqs >= center_guess - search_range) & (freqs <= center_guess + search_range))[0]\n",
    "    if len(idx) == 0:\n",
    "        return signal\n",
    "    true_freq = freqs[idx[np.argmax(psd[idx])]]\n",
    "    w0 = true_freq / (0.5 * fs)\n",
    "    b, a = iirnotch(w0, quality)\n",
    "    return filtfilt(b, a, signal)\n",
    "\n",
    "def parse_xml_annotations(xml_path):\n",
    "    \"\"\"Parse the XML file to extract sleep stage annotations.\"\"\"\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    stages = []\n",
    "    for event in root.findall(\".//ScoredEvent\"):\n",
    "        if event.find(\"EventType\").text == \"Stages|Stages\":\n",
    "            concept = event.find(\"EventConcept\").text.strip().split(\"|\")[-1]  # Get the last part of the stage name\n",
    "            if concept not in ANNOTATION_MAP:\n",
    "                continue\n",
    "            stage_id = ANNOTATION_MAP[concept]\n",
    "            start = float(event.find(\"Start\").text)\n",
    "            duration = float(event.find(\"Duration\").text)\n",
    "\n",
    "            # Split duration into multiple 30-second epochs\n",
    "            num_epochs = int(duration // 30)\n",
    "            for i in range(num_epochs):\n",
    "                epoch_start = start + i * 30\n",
    "                stages.append((epoch_start, 30.0, stage_id))\n",
    "    \n",
    "    return stages\n",
    "\n",
    "def process_record(psg_path, xml_path, channels, target_sfreq, low_freq, high_freq, epoch_length):\n",
    "    \"\"\"Process a single PSG file with its corresponding XML annotations.\"\"\"\n",
    "    raw = mne.io.read_raw_edf(psg_path, preload=True, verbose=False)\n",
    "    if raw.info['sfreq'] != target_sfreq:\n",
    "        raw.resample(target_sfreq, npad=\"auto\", verbose=False)\n",
    "    picks = mne.pick_types(raw.info, eeg=True, eog=True)\n",
    "    if len(picks) < len(channels):\n",
    "        raise ValueError(f\"Not all requested channels found in {psg_path}: {channels}\")\n",
    "    raw.filter(l_freq=low_freq, h_freq=high_freq, picks=picks, verbose=False)\n",
    "\n",
    "    # Apply notch to each picked channel individually (MNE doesn't support adaptive notch out-of-the-box)\n",
    "    for i, ch in enumerate(picks):\n",
    "        signal = raw.get_data(picks=[ch])[0]\n",
    "        filtered = auto_notch_filter(signal, fs=raw.info['sfreq'], center_guess=60.0)\n",
    "        raw._data[ch] = filtered\n",
    "\n",
    "    annotations = parse_xml_annotations(xml_path)\n",
    "\n",
    "    # Convert annotations to MNE-compatible events\n",
    "    events = []\n",
    "    for start, duration, stage_id in annotations:\n",
    "        sample = int(start * raw.info['sfreq'])\n",
    "        events.append([sample, 0, stage_id])\n",
    "\n",
    "    events = np.array(events)\n",
    "    tmin = 0.0\n",
    "    tmax = epoch_length - 1 / raw.info['sfreq']\n",
    "    epochs = mne.Epochs(raw, events=events, event_id=ANNOTATION_MAP, tmin=tmin, tmax=tmax,\n",
    "                        baseline=None, preload=True, verbose=False)\n",
    "    data = epochs.get_data()\n",
    "# Select only desired channels from data\n",
    "data = data[:, channel_indices]\n",
    "    labels = epochs.events[:, -1]\n",
    "\n",
    "    for ch in range(data.shape[1]):\n",
    "        m = np.mean(data[:, ch, :])\n",
    "        s = np.std(data[:, ch, :]) if np.std(data[:, ch, :]) != 0 else 1.0\n",
    "        data[:, ch, :] = (data[:, ch, :] - m) / s\n",
    "    return data, labels, raw.ch_names\n",
    "\n",
    "def create_sequences(data, labels, seq_length, seq_stride):\n",
    "    \"\"\"Create sequences of epochs.\"\"\"\n",
    "    n_epochs = data.shape[0]\n",
    "    sequences, seq_labels = [], []\n",
    "    for start in range(0, n_epochs - seq_length + 1, seq_stride):\n",
    "        sequences.append(data[start:start+seq_length])\n",
    "        seq_labels.append(labels[start:start+seq_length])\n",
    "    return np.array(sequences), np.array(seq_labels)\n",
    "\n",
    "def process_and_save(psg_file, output_dir, channels):\n",
    "    \"\"\"Process and save data for a single PSG file.\"\"\"\n",
    "    rec_id = os.path.basename(psg_file).replace(\".edf\", \"\")\n",
    "    \n",
    "    if rec_id in PROCESSED_ALREADY:\n",
    "        print(f\"Skipping {rec_id}: already processed.\")\n",
    "        return\n",
    "\n",
    "    xml_file = os.path.join(ANNOTATION_DIR, rec_id + \"-nsrr.xml\")\n",
    "    if not os.path.exists(xml_file):\n",
    "        print(f\"Annotation file not found for {psg_file}, skipping.\")\n",
    "        return\n",
    "    try:\n",
    "        data, labels, ch_names = process_record(psg_file, xml_file, CHANNELS_TO_LOAD,\n",
    "                                                TARGET_SFREQ, LOW_FREQ, HIGH_FREQ, EPOCH_LENGTH)\n",
    "        \n",
    "        if data.shape[0] == 0 or labels.shape[0] == 0:\n",
    "            print(f\"No valid epochs for {psg_file}, skipping.\")\n",
    "            return\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {psg_file}: {e}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        np.savez_compressed(os.path.join(output_dir, f\"{rec_id}_epochs.npz\"),\n",
    "                            data=data.astype('float32'), labels=labels.astype('int8'))\n",
    "        sequences, seq_labels = create_sequences(data, labels, SEQ_LENGTH, SEQ_STRIDE)\n",
    "        np.savez_compressed(os.path.join(output_dir, f\"{rec_id}_sequences.npz\"),\n",
    "                            sequences=sequences.astype('float32'), seq_labels=seq_labels.astype('int8'))\n",
    "        print(f\"Processed {rec_id}: epochs {data.shape[0]}, sequences {sequences.shape[0]}, channels: {ch_names}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving {psg_file}: {e}\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ccc8d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 PSG files.\n",
      "Processed mesa-sleep-0167: epochs 1079, sequences 106, channels: ['EKG', 'EOG-L', 'EOG-R', 'EMG', 'EEG1', 'EEG2', 'EEG3', 'Pres', 'Flow', 'Snore', 'Thor', 'Abdo', 'Leg', 'Therm', 'Pos', 'EKG_Off', 'EOG-L_Off', 'EOG-R_Off', 'EMG_Off', 'EEG1_Off', 'EEG2_Off', 'EEG3_Off', 'Pleth', 'OxStatus', 'SpO2', 'HR', 'DHR']\n",
      "Processed mesa-sleep-0359: epochs 1259, sequences 124, channels: ['EKG', 'EOG-L', 'EOG-R', 'EMG', 'EEG1', 'EEG2', 'EEG3', 'Pres', 'Flow', 'Snore', 'Thor', 'Abdo', 'Leg', 'Therm', 'Pos', 'EKG_Off', 'EOG-L_Off', 'EOG-R_Off', 'EMG_Off', 'EEG1_Off', 'EEG2_Off', 'EEG3_Off', 'Pleth', 'OxStatus', 'SpO2', 'HR', 'DHR']\n",
      "Processed mesa-sleep-0601: epochs 1439, sequences 142, channels: ['EKG', 'EOG-L', 'EOG-R', 'EMG', 'EEG1', 'EEG2', 'EEG3', 'Pres', 'Flow', 'Snore', 'Thor', 'Abdo', 'Leg', 'Therm', 'Pos', 'EKG_Off', 'EOG-L_Off', 'EOG-R_Off', 'EMG_Off', 'EEG1_Off', 'EEG2_Off', 'EEG3_Off', 'Pleth', 'OxStatus', 'SpO2', 'HR', 'DHR']\n",
      "Processed mesa-sleep-0629: epochs 1199, sequences 118, channels: ['EKG', 'EOG-L', 'EOG-R', 'EMG', 'EEG1', 'EEG2', 'EEG3', 'Pres', 'Flow', 'Snore', 'Thor', 'Abdo', 'Leg', 'Therm', 'Pos', 'EKG_Off', 'EOG-L_Off', 'EOG-R_Off', 'EMG_Off', 'EEG1_Off', 'EEG2_Off', 'EEG3_Off', 'Pleth', 'OxStatus', 'SpO2', 'HR', 'DHR']\n",
      "Processed mesa-sleep-0211: epochs 1199, sequences 118, channels: ['EKG', 'EOG-L', 'EOG-R', 'EMG', 'EEG1', 'EEG2', 'EEG3', 'Pres', 'Flow', 'Snore', 'Thor', 'Abdo', 'Leg', 'Therm', 'Pos', 'EKG_Off', 'EOG-L_Off', 'EOG-R_Off', 'EMG_Off', 'EEG1_Off', 'EEG2_Off', 'EEG3_Off', 'Pleth', 'OxStatus', 'SpO2', 'HR', 'DHR']\n",
      "Processed mesa-sleep-0563: epochs 1255, sequences 124, channels: ['EKG', 'EOG-L', 'EOG-R', 'EMG', 'EEG1', 'EEG2', 'EEG3', 'Pres', 'Flow', 'Snore', 'Thor', 'Abdo', 'Leg', 'Therm', 'Pos', 'EKG_Off', 'EOG-L_Off', 'EOG-R_Off', 'EMG_Off', 'EEG1_Off', 'EEG2_Off', 'EEG3_Off', 'Pleth', 'OxStatus', 'SpO2', 'HR', 'DHR']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m     Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)(delayed(process_and_save)(f, OUTPUT_DIR, CHANNELS_TO_LOAD) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m psg_files)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m----> 7\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m psg_files \u001b[38;5;241m=\u001b[39m glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(EDF_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.edf\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(psg_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m PSG files.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_and_save\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOUTPUT_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCHANNELS_TO_LOAD\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpsg_files\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    psg_files = glob.glob(os.path.join(EDF_DIR, \"*.edf\"))\n",
    "    print(f\"Found {len(psg_files)} PSG files.\")\n",
    "    Parallel(n_jobs=2)(delayed(process_and_save)(f, OUTPUT_DIR, CHANNELS_TO_LOAD) for f in psg_files)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
