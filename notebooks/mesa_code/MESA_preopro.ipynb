{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b85afebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import mne\n",
    "import xml.etree.ElementTree as ET\n",
    "from scipy.signal import butter, filtfilt, iirnotch, welch\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88668f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "BASE_DIR = \"../../data/MESA\"\n",
    "EDF_DIR = os.path.join(BASE_DIR, 'edfs')\n",
    "ANNOTATION_DIR = os.path.join(BASE_DIR, 'mesa_200_annotations-events-nssr')\n",
    "OUTPUT_DIR = '../../processed_mesa'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "85e7e926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 EDF files to process across specified directories.\n"
     ]
    }
   ],
   "source": [
    "# --- Locate all EDF files across multiple subdirectories ---\n",
    "# List of directory names containing EDFs under BASE_DIR\n",
    "edf_subdir_names = ['mesa_200_subset_edfs'] + [f'mesa_200_subset_edfs {i}' for i in range(2, 19)]\n",
    "all_edf_files = []\n",
    "for subdir_name in edf_subdir_names:\n",
    "    dir_path = os.path.join(EDF_DIR, subdir_name)\n",
    "    if os.path.isdir(dir_path):\n",
    "        edf_files_in_subdir = glob.glob(os.path.join(dir_path, '*.edf')) + \\\n",
    "                              glob.glob(os.path.join(dir_path, '*.EDF'))\n",
    "        all_edf_files.extend(edf_files_in_subdir)\n",
    "    else:\n",
    "        print(f\"Warning: Directory not found - {dir_path}\")\n",
    "\n",
    "# Remove duplicates if any file exists in multiple lists and sort\n",
    "all_edf_files = sorted(list(set(all_edf_files)))\n",
    "\n",
    "print(f\"Found {len(all_edf_files)} EDF files to process across specified directories.\")\n",
    "# --- End of EDF file location ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64871385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EEG 1 is the same as EEG Fz-Cz\n",
    "# EEG 2 is the same as EEG Cz-Oz\n",
    "# EEG 3 is the same as EEG C4 M1\n",
    "# Kim's source: https://sleepdata.org/datasets/mesa/pages/equipment/montage-and-sampling-rate-information.md \n",
    "\n",
    "CHANNELS_TO_LOAD = [\"EEG 1\", \"EOG L\", \"EOG R\"]  # Adjust based on available channels\n",
    "TARGET_SFREQ = 256.0\n",
    "LOW_FREQ = 0.5\n",
    "HIGH_FREQ = 30.0\n",
    "EPOCH_LENGTH = 30.0\n",
    "SEQ_LENGTH = 20\n",
    "SEQ_STRIDE = 10\n",
    "\n",
    "# Sleep stage mapping based on NSRR annotations\n",
    "ANNOTATION_MAP = {\n",
    "    \"0\": 0,  # Wake\n",
    "    \"1\": 1,  # N1\n",
    "    \"2\": 2,  # N2\n",
    "    \"3\": 3,  # N3\n",
    "    \"5\": 4   # REM\n",
    "}\n",
    "\n",
    "def auto_notch_filter(signal, fs, center_guess=60.0, search_range=1.0, quality=30.0):\n",
    "    freqs, psd = welch(signal, fs=fs, nperseg=2048)\n",
    "    idx = np.where((freqs >= center_guess - search_range) & (freqs <= center_guess + search_range))[0]\n",
    "    if len(idx) == 0:\n",
    "        return signal\n",
    "    true_freq = freqs[idx[np.argmax(psd[idx])]]\n",
    "    w0 = true_freq / (0.5 * fs)\n",
    "    b, a = iirnotch(w0, quality)\n",
    "    return filtfilt(b, a, signal)\n",
    "\n",
    "def parse_xml_annotations(xml_path):\n",
    "    \"\"\"Parse the XML file to extract sleep stage annotations.\"\"\"\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    stages = []\n",
    "    for event in root.findall(\".//ScoredEvent\"):\n",
    "        if event.find(\"EventType\").text == \"Stages|Stages\":\n",
    "            concept = event.find(\"EventConcept\").text.strip().split(\"|\")[-1]  # Get the last part of the stage name\n",
    "            if concept not in ANNOTATION_MAP:\n",
    "                continue\n",
    "            stage_id = ANNOTATION_MAP[concept]\n",
    "            start = float(event.find(\"Start\").text)\n",
    "            duration = float(event.find(\"Duration\").text)\n",
    "\n",
    "            # Split duration into multiple 30-second epochs\n",
    "            num_epochs = int(duration // 30)\n",
    "            for i in range(num_epochs):\n",
    "                epoch_start = start + i * 30\n",
    "                stages.append((epoch_start, 30.0, stage_id))\n",
    "    \n",
    "    return stages\n",
    "\n",
    "def process_record(psg_path, xml_path, channels, target_sfreq, low_freq, high_freq, epoch_length):\n",
    "    \"\"\"Process a single PSG file with its corresponding XML annotations.\"\"\"\n",
    "    raw = mne.io.read_raw_edf(psg_path, preload=True, verbose=False)\n",
    "    if raw.info['sfreq'] != target_sfreq:\n",
    "        raw.resample(target_sfreq, npad=\"auto\", verbose=False)\n",
    "    picks = mne.pick_types(raw.info, eeg=True, eog=True)\n",
    "    if len(picks) < len(channels):\n",
    "        raise ValueError(f\"Not all requested channels found in {psg_path}: {channels}\")\n",
    "    raw.filter(l_freq=low_freq, h_freq=high_freq, picks=picks, verbose=False)\n",
    "\n",
    "    # Apply notch to each picked channel individually (MNE doesn't support adaptive notch out-of-the-box)\n",
    "    for i, ch in enumerate(picks):\n",
    "        signal = raw.get_data(picks=[ch])[0]\n",
    "        filtered = auto_notch_filter(signal, fs=raw.info['sfreq'], center_guess=60.0)\n",
    "        raw._data[ch] = filtered\n",
    "\n",
    "    annotations = parse_xml_annotations(xml_path)\n",
    "\n",
    "    # Convert annotations to MNE-compatible events\n",
    "    events = []\n",
    "    for start, duration, stage_id in annotations:\n",
    "        sample = int(start * raw.info['sfreq'])\n",
    "        events.append([sample, 0, stage_id])\n",
    "\n",
    "    events = np.array(events)\n",
    "    tmin = 0.0\n",
    "    tmax = epoch_length - 1 / raw.info['sfreq']\n",
    "    epochs = mne.Epochs(raw, events=events, event_id=ANNOTATION_MAP, tmin=tmin, tmax=tmax,\n",
    "                        baseline=None, preload=True, verbose=False)\n",
    "    data = epochs.get_data()\n",
    "    labels = epochs.events[:, -1]\n",
    "\n",
    "    for ch in range(data.shape[1]):\n",
    "        m = np.mean(data[:, ch, :])\n",
    "        s = np.std(data[:, ch, :]) if np.std(data[:, ch, :]) != 0 else 1.0\n",
    "        data[:, ch, :] = (data[:, ch, :] - m) / s\n",
    "    return data, labels, raw.ch_names\n",
    "\n",
    "def create_sequences(data, labels, seq_length, seq_stride):\n",
    "    \"\"\"Create sequences of epochs.\"\"\"\n",
    "    n_epochs = data.shape[0]\n",
    "    sequences, seq_labels = [], []\n",
    "    for start in range(0, n_epochs - seq_length + 1, seq_stride):\n",
    "        sequences.append(data[start:start+seq_length])\n",
    "        seq_labels.append(labels[start:start+seq_length])\n",
    "    return np.array(sequences), np.array(seq_labels)\n",
    "\n",
    "def process_and_save(psg_file, output_dir, channels):\n",
    "    \"\"\"Process and save data for a single PSG file.\"\"\"\n",
    "    xml_file = os.path.join(ANNOTATION_DIR, os.path.basename(psg_file).replace(\".edf\", \"-nsrr.xml\"))\n",
    "    if not os.path.exists(xml_file):\n",
    "        print(f\"Annotation file not found for {psg_file}, skipping.\")\n",
    "        return\n",
    "    try:\n",
    "        data, labels, ch_names = process_record(psg_file, xml_file, channels,\n",
    "                                                TARGET_SFREQ, LOW_FREQ, HIGH_FREQ, EPOCH_LENGTH)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {psg_file}: {e}\")\n",
    "        return\n",
    "    rec_id = os.path.basename(psg_file).replace(\".edf\", \"\")\n",
    "    np.savez_compressed(os.path.join(output_dir, f\"{rec_id}_epochs.npz\"),\n",
    "                        data=data.astype('float32'), labels=labels.astype('int8'))\n",
    "    sequences, seq_labels = create_sequences(data, labels, SEQ_LENGTH, SEQ_STRIDE)\n",
    "    np.savez_compressed(os.path.join(output_dir, f\"{rec_id}_sequences.npz\"),\n",
    "                        sequences=sequences.astype('float32'), seq_labels=seq_labels.astype('int8'))\n",
    "    print(f\"Processed {rec_id}: epochs {data.shape[0]}, sequences {sequences.shape[0]}, channels: {ch_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ccc8d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 PSG files.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    psg_files = glob.glob(os.path.join(EDF_DIR, \"*.edf\"))\n",
    "    print(f\"Found {len(psg_files)} PSG files.\")\n",
    "    Parallel(n_jobs=2)(delayed(process_and_save)(f, OUTPUT_DIR, CHANNELS_TO_LOAD) for f in psg_files)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
