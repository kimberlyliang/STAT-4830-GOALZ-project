{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, pickle, numpy as np, pandas as pd, pyedflib\n",
    "from scipy.signal import butter, filtfilt, resample_poly, iirnotch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def notch_filter(data, fs, freq=60, Q=30):\n",
    "    b, a = iirnotch(freq, Q, fs)\n",
    "    return filtfilt(b, a, data)\n",
    "\n",
    "def lowpass_filter(data, fs, cutoff=90, order=4):\n",
    "    nyq = fs/2.0\n",
    "    normal_cutoff = cutoff/nyq\n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    return filtfilt(b, a, data)\n",
    "\n",
    "def downsample_window(data, fs, target_fs=200):\n",
    "    return resample_poly(data, target_fs, fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject folder: EPCTL02\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/Users/tereza/spring_2025/STAT_4830/STAT-4830-GOALZ-project/data/ANPHY-Sleep_data\"\n",
    "subject_dirs = sorted([d for d in os.listdir(data_dir)\n",
    "                       if d.lower().startswith(\"epctl\") and os.path.isdir(os.path.join(data_dir, d))])\n",
    "# subject_batch = subject_dirs[5:6]\n",
    "subject_batch = subject_dirs[0:1]\n",
    "subject_batch = subject_dirs[1:2]\n",
    "for subject in subject_batch:\n",
    "    print(\"Subject folder:\", subject)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BATCH 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = \"/Users/tereza/spring_2025/STAT_4830/STAT-4830-GOALZ-project/data/ANPHY-Sleep_data\"\n",
    "# subject_dirs = sorted([d for d in os.listdir(data_dir)\n",
    "#                        if d.lower().startswith(\"epctl\") and os.path.isdir(os.path.join(data_dir, d))])\n",
    "\n",
    "# # Split 93 electrodes into 10 groups and select the first group.\n",
    "# electrode_indices = np.arange(93)\n",
    "# electrode_groups = np.array_split(electrode_indices, 10)\n",
    "# first_group = electrode_groups[0]\n",
    "\n",
    "# subject_batch = subject_dirs[0:1]\n",
    "# # subject_batch = subject_dirs[1:2]\n",
    "# # subject_batch = subject_dirs[2:3]\n",
    "# # subject_batch = subject_dirs[3:4] # sub 4\n",
    "\n",
    "# k = 2  # windows per electrode per subject (so k/2 for \"W\" and k/2 for \"N1\")\n",
    "# n_per_stage = k // 2\n",
    "# window_sec = 2   # window spans 2 sec (±1 sec)\n",
    "# half_window_sec = window_sec / 2\n",
    "\n",
    "# for subject in subject_batch:\n",
    "#     subj_path = os.path.join(data_dir, subject)\n",
    "#     csv_files = glob.glob(os.path.join(subj_path, \"*.csv\"))\n",
    "#     edf_files = glob.glob(os.path.join(subj_path, \"*.edf\"))\n",
    "#     if not csv_files or not edf_files:\n",
    "#         print(f\"Skipping {subject}: missing CSV or EDF.\")\n",
    "#         continue\n",
    "#     df = pd.read_csv(csv_files[0], index_col=0)\n",
    "#     df_W = df[df[\"stage\"]==\"W\"]\n",
    "#     df_N1 = df[df[\"stage\"]==\"N1\"]\n",
    "#     if len(df_W) < n_per_stage or len(df_N1) < n_per_stage:\n",
    "#         print(f\"Skipping {subject}: not enough epochs for one stage.\")\n",
    "#         continue\n",
    "#     # fixed random_state for reproducibility.\n",
    "#     sample_W = df_W.sample(n=n_per_stage, random_state=0)[\"time_index\"].values\n",
    "#     sample_N1 = df_N1.sample(n=n_per_stage, random_state=0)[\"time_index\"].values\n",
    "\n",
    "#     reader = pyedflib.EdfReader(edf_files[0])\n",
    "#     fs = reader.getSampleFrequency(0)  # 1000 Hz\n",
    "#     total_samples = reader.getNSamples()[0]\n",
    "#     n_channels = reader.signals_in_file\n",
    "#     signals = [reader.readSignal(i) for i in range(n_channels)]\n",
    "#     channel_labels = reader.getSignalLabels()\n",
    "#     reader.close()\n",
    "\n",
    "#     subject_windows = {}\n",
    "#     # Process only electrodes in the first group.\n",
    "#     for i in first_group:\n",
    "#         if i >= len(channel_labels):\n",
    "#             continue\n",
    "#         ch_label = channel_labels[i]\n",
    "#         for stage, sample_times in zip([\"W\", \"N1\"], [sample_W, sample_N1]):\n",
    "#             for j, t in enumerate(sample_times):\n",
    "#                 start = int((t - half_window_sec) * fs)\n",
    "#                 end = int((t + half_window_sec) * fs)\n",
    "#                 if start < 0 or end > total_samples:\n",
    "#                     continue\n",
    "#                 win = signals[i][start:end]\n",
    "#                 win = notch_filter(win, fs, freq=60, Q=30)\n",
    "#                 win = lowpass_filter(win, fs, cutoff=90, order=4)\n",
    "#                 ds_win = downsample_window(win, fs, target_fs=200)\n",
    "#                 key = f\"{subject.lower()}_{stage.lower()}_win_{j+1}_{ch_label.replace('-', '').replace(' ', '')}\"\n",
    "#                 # Save both the processed window and the center time index.\n",
    "#                 subject_windows[key] = {\"window\": ds_win, \"time_index\": t}\n",
    "\n",
    "#     out_path = os.path.join(subj_path, \"extracted_windows_group1.pkl\")\n",
    "#     with open(out_path, \"wb\") as f:\n",
    "#         pickle.dump(subject_windows, f)\n",
    "#     print(f\"Saved {len(subject_windows)} windows for {subject} (electrode group 1) at {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BATCH 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = \"/Users/tereza/spring_2025/STAT_4830/STAT-4830-GOALZ-project/data/ANPHY-Sleep_data\"\n",
    "# subject_dirs = sorted([d for d in os.listdir(data_dir)\n",
    "#                        if d.lower().startswith(\"epctl\") and os.path.isdir(os.path.join(data_dir, d))])\n",
    "\n",
    "# # Split 93 electrodes into 10 groups and select the first group.\n",
    "# electrode_indices = np.arange(93)\n",
    "# electrode_groups = np.array_split(electrode_indices, 10)\n",
    "# # first_group = electrode_groups[0]\n",
    "# second_group = electrode_groups[1]\n",
    "\n",
    "# # subject_batch = subject_dirs[3:4] # sub 4 - done\n",
    "# # subject_batch = subject_dirs[4:5]\n",
    "# subject_batch = subject_dirs[5:6]\n",
    "\n",
    "# k = 2  # windows per electrode per subject (so k/2 for \"W\" and k/2 for \"N1\")\n",
    "# n_per_stage = k // 2\n",
    "# window_sec = 2   # window spans 2 sec (±1 sec)\n",
    "# half_window_sec = window_sec / 2\n",
    "\n",
    "# for subject in subject_batch:\n",
    "#     subj_path = os.path.join(data_dir, subject)\n",
    "#     csv_files = glob.glob(os.path.join(subj_path, \"*.csv\"))\n",
    "#     edf_files = glob.glob(os.path.join(subj_path, \"*.edf\"))\n",
    "#     if not csv_files or not edf_files:\n",
    "#         print(f\"Skipping {subject}: missing CSV or EDF.\")\n",
    "#         continue\n",
    "#     df = pd.read_csv(csv_files[0], index_col=0)\n",
    "#     df_W = df[df[\"stage\"]==\"W\"]\n",
    "#     df_N1 = df[df[\"stage\"]==\"N1\"]\n",
    "#     if len(df_W) < n_per_stage or len(df_N1) < n_per_stage:\n",
    "#         print(f\"Skipping {subject}: not enough epochs for one stage.\")\n",
    "#         continue\n",
    "#     # fixed random_state for reproducibility.\n",
    "#     sample_W = df_W.sample(n=n_per_stage, random_state=0)[\"time_index\"].values\n",
    "#     sample_N1 = df_N1.sample(n=n_per_stage, random_state=0)[\"time_index\"].values\n",
    "\n",
    "#     reader = pyedflib.EdfReader(edf_files[0])\n",
    "#     fs = reader.getSampleFrequency(0)  # 1000 Hz\n",
    "#     total_samples = reader.getNSamples()[0]\n",
    "#     n_channels = reader.signals_in_file\n",
    "#     signals = [reader.readSignal(i) for i in range(n_channels)]\n",
    "#     channel_labels = reader.getSignalLabels()\n",
    "#     reader.close()\n",
    "\n",
    "#     subject_windows = {}\n",
    "#     # Process only electrodes in the first group.\n",
    "#     for i in second_group:\n",
    "#         if i >= len(channel_labels):\n",
    "#             continue\n",
    "#         ch_label = channel_labels[i]\n",
    "#         for stage, sample_times in zip([\"W\", \"N1\"], [sample_W, sample_N1]):\n",
    "#             for j, t in enumerate(sample_times):\n",
    "#                 start = int((t - half_window_sec) * fs)\n",
    "#                 end = int((t + half_window_sec) * fs)\n",
    "#                 if start < 0 or end > total_samples:\n",
    "#                     continue\n",
    "#                 win = signals[i][start:end]\n",
    "#                 win = notch_filter(win, fs, freq=60, Q=30)\n",
    "#                 win = lowpass_filter(win, fs, cutoff=90, order=4)\n",
    "#                 ds_win = downsample_window(win, fs, target_fs=200)\n",
    "#                 key = f\"{subject.lower()}_{stage.lower()}_win_{j+1}_{ch_label.replace('-', '').replace(' ', '')}\"\n",
    "#                 # Save both the processed window and the center time index.\n",
    "#                 subject_windows[key] = {\"window\": ds_win, \"time_index\": t}\n",
    "\n",
    "#     out_path = os.path.join(subj_path, \"extracted_windows_group2.pkl\")\n",
    "#     with open(out_path, \"wb\") as f:\n",
    "#         pickle.dump(subject_windows, f)\n",
    "#     print(f\"Saved {len(subject_windows)} windows for {subject} (electrode group 2) at {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BATCH 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = \"/Users/tereza/spring_2025/STAT_4830/STAT-4830-GOALZ-project/data/ANPHY-Sleep_data\"\n",
    "# subject_dirs = sorted([d for d in os.listdir(data_dir)\n",
    "#                        if d.lower().startswith(\"epctl\") and os.path.isdir(os.path.join(data_dir, d))])\n",
    "\n",
    "# # Split 93 electrodes into 10 groups and select the first group.\n",
    "# electrode_indices = np.arange(93)\n",
    "# electrode_groups = np.array_split(electrode_indices, 10)\n",
    "# # first_group = electrode_groups[0]\n",
    "# group = electrode_groups[2]\n",
    "\n",
    "# # subject_batch = subject_dirs[6:7] \n",
    "# # subject_batch = subject_dirs[7:8]\n",
    "# subject_batch = subject_dirs[8:9]\n",
    "\n",
    "# k = 2  # windows per electrode per subject (so k/2 for \"W\" and k/2 for \"N1\")\n",
    "# n_per_stage = k // 2\n",
    "# window_sec = 2   # window spans 2 sec (±1 sec)\n",
    "# half_window_sec = window_sec / 2\n",
    "\n",
    "# for subject in subject_batch:\n",
    "#     subj_path = os.path.join(data_dir, subject)\n",
    "#     csv_files = glob.glob(os.path.join(subj_path, \"*.csv\"))\n",
    "#     edf_files = glob.glob(os.path.join(subj_path, \"*.edf\"))\n",
    "#     if not csv_files or not edf_files:\n",
    "#         print(f\"Skipping {subject}: missing CSV or EDF.\")\n",
    "#         continue\n",
    "#     df = pd.read_csv(csv_files[0], index_col=0)\n",
    "#     df_W = df[df[\"stage\"]==\"W\"]\n",
    "#     df_N1 = df[df[\"stage\"]==\"N1\"]\n",
    "#     if len(df_W) < n_per_stage or len(df_N1) < n_per_stage:\n",
    "#         print(f\"Skipping {subject}: not enough epochs for one stage.\")\n",
    "#         continue\n",
    "#     sample_W = df_W.sample(n=n_per_stage, random_state=1)[\"time_index\"].values\n",
    "#     sample_N1 = df_N1.sample(n=n_per_stage, random_state=1)[\"time_index\"].values\n",
    "\n",
    "#     reader = pyedflib.EdfReader(edf_files[0])\n",
    "#     fs = reader.getSampleFrequency(0)  # 1000 Hz\n",
    "#     total_samples = reader.getNSamples()[0]\n",
    "#     n_channels = reader.signals_in_file\n",
    "#     signals = [reader.readSignal(i) for i in range(n_channels)]\n",
    "#     channel_labels = reader.getSignalLabels()\n",
    "#     reader.close()\n",
    "\n",
    "#     subject_windows = {}\n",
    "#     # Process only electrodes in the first group.\n",
    "#     for i in group:\n",
    "#         if i >= len(channel_labels):\n",
    "#             continue\n",
    "#         ch_label = channel_labels[i]\n",
    "#         for stage, sample_times in zip([\"W\", \"N1\"], [sample_W, sample_N1]):\n",
    "#             for j, t in enumerate(sample_times):\n",
    "#                 start = int((t - half_window_sec) * fs)\n",
    "#                 end = int((t + half_window_sec) * fs)\n",
    "#                 if start < 0 or end > total_samples:\n",
    "#                     continue\n",
    "#                 win = signals[i][start:end]\n",
    "#                 win = notch_filter(win, fs, freq=60, Q=30)\n",
    "#                 win = lowpass_filter(win, fs, cutoff=90, order=4)\n",
    "#                 ds_win = downsample_window(win, fs, target_fs=200)\n",
    "#                 key = f\"{subject.lower()}_{stage.lower()}_win_{j+1}_{ch_label.replace('-', '').replace(' ', '')}\"\n",
    "#                 # Save both the processed window and the center time index.\n",
    "#                 subject_windows[key] = {\"window\": ds_win, \"time_index\": t}\n",
    "\n",
    "#     out_path = os.path.join(subj_path, \"extracted_windows_group3.pkl\")\n",
    "#     with open(out_path, \"wb\") as f:\n",
    "#         pickle.dump(subject_windows, f)\n",
    "#     print(f\"Saved {len(subject_windows)} windows for {subject} (electrode group 3) at {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BATCH 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = \"/Users/tereza/spring_2025/STAT_4830/STAT-4830-GOALZ-project/data/ANPHY-Sleep_data\"\n",
    "# subject_dirs = sorted([d for d in os.listdir(data_dir)\n",
    "#                        if d.lower().startswith(\"epctl\") and os.path.isdir(os.path.join(data_dir, d))])\n",
    "\n",
    "# # Split 93 electrodes into 10 groups and select the first group.\n",
    "# electrode_indices = np.arange(93)\n",
    "# electrode_groups = np.array_split(electrode_indices, 10)\n",
    "# # first_group = electrode_groups[0]\n",
    "# group = electrode_groups[3]\n",
    "\n",
    "# # subject_batch = subject_dirs[9:10] # done\n",
    "# # subject_batch = subject_dirs[10:11]\n",
    "# subject_batch = subject_dirs[11:12]\n",
    "\n",
    "# k = 2  # windows per electrode per subject (so k/2 for \"W\" and k/2 for \"N1\")\n",
    "# n_per_stage = k // 2\n",
    "# window_sec = 2   # window spans 2 sec (±1 sec)\n",
    "# half_window_sec = window_sec / 2\n",
    "\n",
    "# for subject in subject_batch:\n",
    "#     subj_path = os.path.join(data_dir, subject)\n",
    "#     csv_files = glob.glob(os.path.join(subj_path, \"*.csv\"))\n",
    "#     edf_files = glob.glob(os.path.join(subj_path, \"*.edf\"))\n",
    "#     if not csv_files or not edf_files:\n",
    "#         print(f\"Skipping {subject}: missing CSV or EDF.\")\n",
    "#         continue\n",
    "#     df = pd.read_csv(csv_files[0], index_col=0)\n",
    "#     df_W = df[df[\"stage\"]==\"W\"]\n",
    "#     df_N1 = df[df[\"stage\"]==\"N1\"]\n",
    "#     if len(df_W) < n_per_stage or len(df_N1) < n_per_stage:\n",
    "#         print(f\"Skipping {subject}: not enough epochs for one stage.\")\n",
    "#         continue\n",
    "#     # fixed random_state for reproducibility.\n",
    "#     sample_W = df_W.sample(n=n_per_stage, random_state=2)[\"time_index\"].values\n",
    "#     sample_N1 = df_N1.sample(n=n_per_stage, random_state=2)[\"time_index\"].values\n",
    "\n",
    "#     reader = pyedflib.EdfReader(edf_files[0])\n",
    "#     fs = reader.getSampleFrequency(0)  # 1000 Hz\n",
    "#     total_samples = reader.getNSamples()[0]\n",
    "#     n_channels = reader.signals_in_file\n",
    "#     signals = [reader.readSignal(i) for i in range(n_channels)]\n",
    "#     channel_labels = reader.getSignalLabels()\n",
    "#     reader.close()\n",
    "\n",
    "#     subject_windows = {}\n",
    "#     for i in group:\n",
    "#         if i >= len(channel_labels):\n",
    "#             continue\n",
    "#         ch_label = channel_labels[i]\n",
    "#         for stage, sample_times in zip([\"W\", \"N1\"], [sample_W, sample_N1]):\n",
    "#             for j, t in enumerate(sample_times):\n",
    "#                 start = int((t - half_window_sec) * fs)\n",
    "#                 end = int((t + half_window_sec) * fs)\n",
    "#                 if start < 0 or end > total_samples:\n",
    "#                     continue\n",
    "#                 win = signals[i][start:end]\n",
    "#                 win = notch_filter(win, fs, freq=60, Q=30)\n",
    "#                 win = lowpass_filter(win, fs, cutoff=90, order=4)\n",
    "#                 ds_win = downsample_window(win, fs, target_fs=200)\n",
    "#                 key = f\"{subject.lower()}_{stage.lower()}_win_{j+1}_{ch_label.replace('-', '').replace(' ', '')}\"\n",
    "#                 # Save both the processed window and the center time index.\n",
    "#                 subject_windows[key] = {\"window\": ds_win, \"time_index\": t}\n",
    "\n",
    "#     out_path = os.path.join(subj_path, \"extracted_windows_group4.pkl\")\n",
    "#     with open(out_path, \"wb\") as f:\n",
    "#         pickle.dump(subject_windows, f)\n",
    "#     print(f\"Saved {len(subject_windows)} windows for {subject} (electrode group 4) at {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BATCH 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = \"/Users/tereza/spring_2025/STAT_4830/STAT-4830-GOALZ-project/data/ANPHY-Sleep_data\"\n",
    "# subject_dirs = sorted([d for d in os.listdir(data_dir)\n",
    "#                        if d.lower().startswith(\"epctl\") and os.path.isdir(os.path.join(data_dir, d))])\n",
    "\n",
    "# # Split 93 electrodes into 10 groups and select the first group.\n",
    "# electrode_indices = np.arange(93)\n",
    "# electrode_groups = np.array_split(electrode_indices, 10)\n",
    "# # first_group = electrode_groups[0]\n",
    "# group = electrode_groups[4]\n",
    "\n",
    "# # subject_batch = subject_dirs[12:13] # done\n",
    "# # subject_batch = subject_dirs[13:14] # done \n",
    "# subject_batch = subject_dirs[14:15] # done\n",
    "\n",
    "# k = 2  # windows per electrode per subject (so k/2 for \"W\" and k/2 for \"N1\")\n",
    "# n_per_stage = k // 2\n",
    "# window_sec = 2   # window spans 2 sec (±1 sec)\n",
    "# half_window_sec = window_sec / 2\n",
    "\n",
    "# for subject in subject_batch:\n",
    "#     subj_path = os.path.join(data_dir, subject)\n",
    "#     csv_files = glob.glob(os.path.join(subj_path, \"*.csv\"))\n",
    "#     edf_files = glob.glob(os.path.join(subj_path, \"*.edf\"))\n",
    "#     if not csv_files or not edf_files:\n",
    "#         print(f\"Skipping {subject}: missing CSV or EDF.\")\n",
    "#         continue\n",
    "#     df = pd.read_csv(csv_files[0], index_col=0)\n",
    "#     df_W = df[df[\"stage\"]==\"W\"]\n",
    "#     df_N1 = df[df[\"stage\"]==\"N1\"]\n",
    "#     if len(df_W) < n_per_stage or len(df_N1) < n_per_stage:\n",
    "#         print(f\"Skipping {subject}: not enough epochs for one stage.\")\n",
    "#         continue\n",
    "#     # fixed random_state\n",
    "#     sample_W = df_W.sample(n=n_per_stage, random_state=3)[\"time_index\"].values\n",
    "#     sample_N1 = df_N1.sample(n=n_per_stage, random_state=3)[\"time_index\"].values\n",
    "\n",
    "#     reader = pyedflib.EdfReader(edf_files[0])\n",
    "#     fs = reader.getSampleFrequency(0)  # 1000 Hz\n",
    "#     total_samples = reader.getNSamples()[0]\n",
    "#     n_channels = reader.signals_in_file\n",
    "#     signals = [reader.readSignal(i) for i in range(n_channels)]\n",
    "#     channel_labels = reader.getSignalLabels()\n",
    "#     reader.close()\n",
    "\n",
    "#     subject_windows = {}\n",
    "#     for i in group:\n",
    "#         if i >= len(channel_labels):\n",
    "#             continue\n",
    "#         ch_label = channel_labels[i]\n",
    "#         for stage, sample_times in zip([\"W\", \"N1\"], [sample_W, sample_N1]):\n",
    "#             for j, t in enumerate(sample_times):\n",
    "#                 start = int((t - half_window_sec) * fs)\n",
    "#                 end = int((t + half_window_sec) * fs)\n",
    "#                 if start < 0 or end > total_samples:\n",
    "#                     continue\n",
    "#                 win = signals[i][start:end]\n",
    "#                 win = notch_filter(win, fs, freq=60, Q=30)\n",
    "#                 win = lowpass_filter(win, fs, cutoff=90, order=4)\n",
    "#                 ds_win = downsample_window(win, fs, target_fs=200)\n",
    "#                 key = f\"{subject.lower()}_{stage.lower()}_win_{j+1}_{ch_label.replace('-', '').replace(' ', '')}\"\n",
    "#                 # Save both the processed window and the center time index.\n",
    "#                 subject_windows[key] = {\"window\": ds_win, \"time_index\": t}\n",
    "\n",
    "#     out_path = os.path.join(subj_path, \"extracted_windows_group5.pkl\")\n",
    "#     with open(out_path, \"wb\") as f:\n",
    "#         pickle.dump(subject_windows, f)\n",
    "#     print(f\"Saved {len(subject_windows)} windows for {subject} (electrode group 5) at {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subj_path = \"/Users/tereza/spring_2025/STAT_4830/STAT-4830-GOALZ-project/data/ANPHY-Sleep_data/epctl01\"  \n",
    "\n",
    "# pkl_path = os.path.join(subj_path, \"extracted_windows_group1.pkl\")\n",
    "\n",
    "# with open(pkl_path, \"rb\") as f:\n",
    "#     windows_dict = pickle.load(f)\n",
    "\n",
    "# print(\"Total number of windows saved:\", len(windows_dict))\n",
    "# print(\"\\nSample entries:\")\n",
    "# for key in list(windows_dict.keys())[:5]:\n",
    "#     data = windows_dict[key]\n",
    "#     print(f\"Key: {key}\")\n",
    "#     print(\"  Window shape:\", data[\"window\"].shape)\n",
    "#     print(\"  Center time index:\", data[\"time_index\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BATCH 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = \"/Users/tereza/spring_2025/STAT_4830/STAT-4830-GOALZ-project/data/ANPHY-Sleep_data\"\n",
    "# subject_dirs = sorted([d for d in os.listdir(data_dir)\n",
    "#                        if d.lower().startswith(\"epctl\") and os.path.isdir(os.path.join(data_dir, d))])\n",
    "\n",
    "# # Split 93 electrodes into 10 groups and select the first group.\n",
    "# electrode_indices = np.arange(93)\n",
    "# electrode_groups = np.array_split(electrode_indices, 10)\n",
    "# # first_group = electrode_groups[0]\n",
    "# group = electrode_groups[5]\n",
    "\n",
    "# # subject_batch = subject_dirs[15:16] \n",
    "# # subject_batch = subject_dirs[16:17] # done\n",
    "# subject_batch = subject_dirs[17:18] # done\n",
    "\n",
    "# k = 2  # windows per electrode per subject (so k/2 for \"W\" and k/2 for \"N1\")\n",
    "# n_per_stage = k // 2\n",
    "# window_sec = 2   # window spans 2 sec (±1 sec)\n",
    "# half_window_sec = window_sec / 2\n",
    "\n",
    "# for subject in subject_batch:\n",
    "#     subj_path = os.path.join(data_dir, subject)\n",
    "#     csv_files = glob.glob(os.path.join(subj_path, \"*.csv\"))\n",
    "#     edf_files = glob.glob(os.path.join(subj_path, \"*.edf\"))\n",
    "#     if not csv_files or not edf_files:\n",
    "#         print(f\"Skipping {subject}: missing CSV or EDF.\")\n",
    "#         continue\n",
    "#     df = pd.read_csv(csv_files[0], index_col=0)\n",
    "#     df_W = df[df[\"stage\"]==\"W\"]\n",
    "#     df_N1 = df[df[\"stage\"]==\"N1\"]\n",
    "#     if len(df_W) < n_per_stage or len(df_N1) < n_per_stage:\n",
    "#         print(f\"Skipping {subject}: not enough epochs for one stage.\")\n",
    "#         continue\n",
    "#     # fixed random_state\n",
    "#     sample_W = df_W.sample(n=n_per_stage, random_state=3)[\"time_index\"].values\n",
    "#     sample_N1 = df_N1.sample(n=n_per_stage, random_state=3)[\"time_index\"].values\n",
    "\n",
    "#     reader = pyedflib.EdfReader(edf_files[0])\n",
    "#     fs = reader.getSampleFrequency(0)  # 1000 Hz\n",
    "#     total_samples = reader.getNSamples()[0]\n",
    "#     n_channels = reader.signals_in_file\n",
    "#     signals = [reader.readSignal(i) for i in range(n_channels)]\n",
    "#     channel_labels = reader.getSignalLabels()\n",
    "#     reader.close()\n",
    "\n",
    "#     subject_windows = {}\n",
    "#     for i in group:\n",
    "#         if i >= len(channel_labels):\n",
    "#             continue\n",
    "#         ch_label = channel_labels[i]\n",
    "#         for stage, sample_times in zip([\"W\", \"N1\"], [sample_W, sample_N1]):\n",
    "#             for j, t in enumerate(sample_times):\n",
    "#                 start = int((t - half_window_sec) * fs)\n",
    "#                 end = int((t + half_window_sec) * fs)\n",
    "#                 if start < 0 or end > total_samples:\n",
    "#                     continue\n",
    "#                 win = signals[i][start:end]\n",
    "#                 win = notch_filter(win, fs, freq=60, Q=30)\n",
    "#                 win = lowpass_filter(win, fs, cutoff=90, order=4)\n",
    "#                 ds_win = downsample_window(win, fs, target_fs=200)\n",
    "#                 key = f\"{subject.lower()}_{stage.lower()}_win_{j+1}_{ch_label.replace('-', '').replace(' ', '')}\"\n",
    "#                 # Save both the processed window and the center time index.\n",
    "#                 subject_windows[key] = {\"window\": ds_win, \"time_index\": t}\n",
    "\n",
    "#     out_path = os.path.join(subj_path, \"extracted_windows_group6.pkl\")\n",
    "#     with open(out_path, \"wb\") as f:\n",
    "#         pickle.dump(subject_windows, f)\n",
    "#     print(f\"Saved {len(subject_windows)} windows for {subject} (electrode group 6) at {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLOTTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subj_path = \"/Users/tereza/spring_2025/STAT_4830/STAT-4830-GOALZ-project/data/ANPHY-Sleep_data/epctl18\"\n",
    "# pkl_path = os.path.join(subj_path, \"extracted_windows_group6.pkl\")\n",
    "\n",
    "# with open(pkl_path, \"rb\") as f:\n",
    "#     windows_dict = pickle.load(f)\n",
    "\n",
    "# # Print all keys to verify available electrodes\n",
    "# all_keys = list(windows_dict.keys())\n",
    "# print(\"All keys in pkl:\", all_keys)\n",
    "\n",
    "# # For example, search for keys containing \"p1ref\" (not \"fp1ref\")\n",
    "# w_keys = [key for key in all_keys if \"p1ref\" in key.lower() and \"_w_\" in key.lower()]\n",
    "# n1_keys = [key for key in all_keys if \"p1ref\" in key.lower() and \"_n1_\" in key.lower()]\n",
    "\n",
    "# print(\"W keys for p1ref:\", w_keys)\n",
    "# print(\"N1 keys for p1ref:\", n1_keys)\n",
    "\n",
    "# if w_keys and n1_keys:\n",
    "#     window_w = windows_dict[w_keys[0]][\"window\"]\n",
    "#     window_n1 = windows_dict[n1_keys[0]][\"window\"]\n",
    "    \n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#     plt.plot(window_w, label=\"Wake (W)\")\n",
    "#     plt.plot(window_n1, label=\"N1\")\n",
    "#     plt.xlabel(\"Sample Index (at 200 Hz)\")\n",
    "#     plt.ylabel(\"Amplitude\")\n",
    "#     plt.title(\"2-second Windows for electrode P1Ref\")\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "# else:\n",
    "#     print(\"Could not find both W and N1 windows for electrode P1Ref\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subj_path = \"/Users/tereza/spring_2025/STAT_4830/STAT-4830-GOALZ-project/data/ANPHY-Sleep_data/epctl18\"\n",
    "# pkl_path = os.path.join(subj_path, \"extracted_windows_group6.pkl\")\n",
    "\n",
    "# with open(pkl_path, \"rb\") as f:\n",
    "#     windows_dict = pickle.load(f)\n",
    "\n",
    "# # Get unique electrode labels from the keys.\n",
    "# unique_electrodes = set()\n",
    "# for key in windows_dict.keys():\n",
    "#     # Assuming the electrode label is the last part of the key separated by underscores.\n",
    "#     elec_label = key.split(\"_\")[-1]\n",
    "#     unique_electrodes.add(elec_label)\n",
    "\n",
    "# unique_electrodes = sorted(unique_electrodes)\n",
    "# print(\"Unique electrode labels:\", unique_electrodes)\n",
    "\n",
    "# # For each electrode, find the corresponding W and N1 keys and plot their windows.\n",
    "# for elec in unique_electrodes:\n",
    "#     # Find keys that contain the electrode label and stage.\n",
    "#     w_keys = [k for k in windows_dict if elec.lower() in k.lower() and \"_w_\" in k.lower()]\n",
    "#     n1_keys = [k for k in windows_dict if elec.lower() in k.lower() and \"_n1_\" in k.lower()]\n",
    "    \n",
    "#     if w_keys and n1_keys:\n",
    "#         window_w = windows_dict[w_keys[0]][\"window\"]\n",
    "#         window_n1 = windows_dict[n1_keys[0]][\"window\"]\n",
    "        \n",
    "#         plt.figure(figsize=(8, 4))\n",
    "#         plt.plot(window_w, marker='o', linestyle='-', label=\"Wake (W)\")\n",
    "#         plt.plot(window_n1, marker='o', linestyle='-', label=\"N1\")\n",
    "#         plt.xlabel(\"Sample Index (at 200 Hz)\")\n",
    "#         plt.ylabel(\"Amplitude\")\n",
    "#         plt.title(f\"2-second Windows for electrode {elec}\")\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "#     else:\n",
    "#         print(f\"Missing windows for electrode: {elec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subj_path = \"/Users/tereza/spring_2025/STAT_4830/STAT-4830-GOALZ-project/data/ANPHY-Sleep_data/epctl18\"\n",
    "# pkl_path = os.path.join(subj_path, \"extracted_windows_group6.pkl\")\n",
    "\n",
    "# with open(pkl_path, \"rb\") as f:\n",
    "#     windows_dict = pickle.load(f)\n",
    "\n",
    "# # Collect unique electrode labels (last underscore-separated part of each key).\n",
    "# unique_electrodes = set()\n",
    "# for key in windows_dict.keys():\n",
    "#     elec_label = key.split(\"_\")[-1]\n",
    "#     unique_electrodes.add(elec_label)\n",
    "\n",
    "# unique_electrodes = sorted(unique_electrodes)\n",
    "\n",
    "# for elec in unique_electrodes:\n",
    "#     # Find the key for W\n",
    "#     w_keys = [k for k in windows_dict if elec.lower() in k.lower() and \"_w_\" in k.lower()]\n",
    "#     # Find the key for N1\n",
    "#     n1_keys = [k for k in windows_dict if elec.lower() in k.lower() and \"_n1_\" in k.lower()]\n",
    "\n",
    "#     if w_keys and n1_keys:\n",
    "#         w_key = w_keys[0]\n",
    "#         n1_key = n1_keys[0]\n",
    "#         w_time = windows_dict[w_key][\"time_index\"]\n",
    "#         n1_time = windows_dict[n1_key][\"time_index\"]\n",
    "#         print(f\"Electrode: {elec}\")\n",
    "#         print(f\"  Wake clip centered at t={w_time:.2f} s (key: {w_key})\")\n",
    "#         print(f\"  N1 clip centered at t={n1_time:.2f} s (key: {n1_key})\")\n",
    "#     else:\n",
    "#         print(f\"Electrode: {elec} has incomplete data (missing W or N1).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BATCH 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 18 windows for EPCTL21 (electrode group 7) at /Users/tereza/spring_2025/STAT_4830/STAT-4830-GOALZ-project/data/ANPHY-Sleep_data/EPCTL21/extracted_windows_group7.pkl\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/Users/tereza/spring_2025/STAT_4830/STAT-4830-GOALZ-project/data/ANPHY-Sleep_data\"\n",
    "subject_dirs = sorted([d for d in os.listdir(data_dir)\n",
    "                       if d.lower().startswith(\"epctl\") and os.path.isdir(os.path.join(data_dir, d))])\n",
    "\n",
    "# Split 93 electrodes into 10 groups and select the first group.\n",
    "electrode_indices = np.arange(93)\n",
    "electrode_groups = np.array_split(electrode_indices, 10)\n",
    "# first_group = electrode_groups[0]\n",
    "group = electrode_groups[6]\n",
    "\n",
    "# subject_batch = subject_dirs[18:19] # 12:12 done\n",
    "# subject_batch = subject_dirs[19:20] # done\n",
    "subject_batch = subject_dirs[20:21] # done\n",
    "\n",
    "k = 2  # windows per electrode per subject (so k/2 for \"W\" and k/2 for \"N1\")\n",
    "n_per_stage = k // 2\n",
    "window_sec = 2   # window spans 2 sec (±1 sec)\n",
    "half_window_sec = window_sec / 2\n",
    "\n",
    "for subject in subject_batch:\n",
    "    subj_path = os.path.join(data_dir, subject)\n",
    "    csv_files = glob.glob(os.path.join(subj_path, \"*.csv\"))\n",
    "    edf_files = glob.glob(os.path.join(subj_path, \"*.edf\"))\n",
    "    if not csv_files or not edf_files:\n",
    "        print(f\"Skipping {subject}: missing CSV or EDF.\")\n",
    "        continue\n",
    "    df = pd.read_csv(csv_files[0], index_col=0)\n",
    "    df_W = df[df[\"stage\"]==\"W\"]\n",
    "    df_N1 = df[df[\"stage\"]==\"N1\"]\n",
    "    if len(df_W) < n_per_stage or len(df_N1) < n_per_stage:\n",
    "        print(f\"Skipping {subject}: not enough epochs for one stage.\")\n",
    "        continue\n",
    "    # fixed random_state\n",
    "    sample_W = df_W.sample(n=n_per_stage, random_state=3)[\"time_index\"].values\n",
    "    sample_N1 = df_N1.sample(n=n_per_stage, random_state=3)[\"time_index\"].values\n",
    "\n",
    "    reader = pyedflib.EdfReader(edf_files[0])\n",
    "    fs = reader.getSampleFrequency(0)  # 1000 Hz\n",
    "    total_samples = reader.getNSamples()[0]\n",
    "    n_channels = reader.signals_in_file\n",
    "    signals = [reader.readSignal(i) for i in range(n_channels)]\n",
    "    channel_labels = reader.getSignalLabels()\n",
    "    reader.close()\n",
    "\n",
    "    subject_windows = {}\n",
    "    for i in group:\n",
    "        if i >= len(channel_labels):\n",
    "            continue\n",
    "        ch_label = channel_labels[i]\n",
    "        for stage, sample_times in zip([\"W\", \"N1\"], [sample_W, sample_N1]):\n",
    "            for j, t in enumerate(sample_times):\n",
    "                start = int((t - half_window_sec) * fs)\n",
    "                end = int((t + half_window_sec) * fs)\n",
    "                if start < 0 or end > total_samples:\n",
    "                    continue\n",
    "                win = signals[i][start:end]\n",
    "                win = notch_filter(win, fs, freq=60, Q=30)\n",
    "                win = lowpass_filter(win, fs, cutoff=90, order=4)\n",
    "                ds_win = downsample_window(win, fs, target_fs=200)\n",
    "                key = f\"{subject.lower()}_{stage.lower()}_win_{j+1}_{ch_label.replace('-', '').replace(' ', '')}\"\n",
    "                # Save both the processed window and the center time index.\n",
    "                subject_windows[key] = {\"window\": ds_win, \"time_index\": t}\n",
    "\n",
    "    out_path = os.path.join(subj_path, \"extracted_windows_group7.pkl\")\n",
    "    with open(out_path, \"wb\") as f:\n",
    "        pickle.dump(subject_windows, f)\n",
    "    print(f\"Saved {len(subject_windows)} windows for {subject} (electrode group 7) at {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BATCH 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = \"/Users/tereza/spring_2025/STAT_4830/STAT-4830-GOALZ-project/data/ANPHY-Sleep_data\"\n",
    "# subject_dirs = sorted([d for d in os.listdir(data_dir)\n",
    "#                        if d.lower().startswith(\"epctl\") and os.path.isdir(os.path.join(data_dir, d))])\n",
    "\n",
    "# # Split 93 electrodes into 10 groups and select the first group.\n",
    "# electrode_indices = np.arange(93)\n",
    "# electrode_groups = np.array_split(electrode_indices, 10)\n",
    "# # first_group = electrode_groups[0]\n",
    "# group = electrode_groups[7]\n",
    "\n",
    "# subject_batch = subject_dirs[21:22] \n",
    "# # subject_batch = subject_dirs[22:23]\n",
    "# # subject_batch = subject_dirs[23:24]\n",
    "\n",
    "# k = 2  # windows per electrode per subject (so k/2 for \"W\" and k/2 for \"N1\")\n",
    "# n_per_stage = k // 2\n",
    "# window_sec = 2   # window spans 2 sec (±1 sec)\n",
    "# half_window_sec = window_sec / 2\n",
    "\n",
    "# for subject in subject_batch:\n",
    "#     subj_path = os.path.join(data_dir, subject)\n",
    "#     csv_files = glob.glob(os.path.join(subj_path, \"*.csv\"))\n",
    "#     edf_files = glob.glob(os.path.join(subj_path, \"*.edf\"))\n",
    "#     if not csv_files or not edf_files:\n",
    "#         print(f\"Skipping {subject}: missing CSV or EDF.\")\n",
    "#         continue\n",
    "#     df = pd.read_csv(csv_files[0], index_col=0)\n",
    "#     df_W = df[df[\"stage\"]==\"W\"]\n",
    "#     df_N1 = df[df[\"stage\"]==\"N1\"]\n",
    "#     if len(df_W) < n_per_stage or len(df_N1) < n_per_stage:\n",
    "#         print(f\"Skipping {subject}: not enough epochs for one stage.\")\n",
    "#         continue\n",
    "#     # fixed random_state\n",
    "#     sample_W = df_W.sample(n=n_per_stage, random_state=3)[\"time_index\"].values\n",
    "#     sample_N1 = df_N1.sample(n=n_per_stage, random_state=3)[\"time_index\"].values\n",
    "\n",
    "#     reader = pyedflib.EdfReader(edf_files[0])\n",
    "#     fs = reader.getSampleFrequency(0)  # 1000 Hz\n",
    "#     total_samples = reader.getNSamples()[0]\n",
    "#     n_channels = reader.signals_in_file\n",
    "#     signals = [reader.readSignal(i) for i in range(n_channels)]\n",
    "#     channel_labels = reader.getSignalLabels()\n",
    "#     reader.close()\n",
    "\n",
    "#     subject_windows = {}\n",
    "#     for i in group:\n",
    "#         if i >= len(channel_labels):\n",
    "#             continue\n",
    "#         ch_label = channel_labels[i]\n",
    "#         for stage, sample_times in zip([\"W\", \"N1\"], [sample_W, sample_N1]):\n",
    "#             for j, t in enumerate(sample_times):\n",
    "#                 start = int((t - half_window_sec) * fs)\n",
    "#                 end = int((t + half_window_sec) * fs)\n",
    "#                 if start < 0 or end > total_samples:\n",
    "#                     continue\n",
    "#                 win = signals[i][start:end]\n",
    "#                 win = notch_filter(win, fs, freq=60, Q=30)\n",
    "#                 win = lowpass_filter(win, fs, cutoff=90, order=4)\n",
    "#                 ds_win = downsample_window(win, fs, target_fs=200)\n",
    "#                 key = f\"{subject.lower()}_{stage.lower()}_win_{j+1}_{ch_label.replace('-', '').replace(' ', '')}\"\n",
    "#                 # Save both the processed window and the center time index.\n",
    "#                 subject_windows[key] = {\"window\": ds_win, \"time_index\": t}\n",
    "\n",
    "#     out_path = os.path.join(subj_path, \"extracted_windows_group8.pkl\")\n",
    "#     with open(out_path, \"wb\") as f:\n",
    "#         pickle.dump(subject_windows, f)\n",
    "#     print(f\"Saved {len(subject_windows)} windows for {subject} (electrode group 8) at {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BATCH 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = \"/Users/tereza/spring_2025/STAT_4830/STAT-4830-GOALZ-project/data/ANPHY-Sleep_data\"\n",
    "# subject_dirs = sorted([d for d in os.listdir(data_dir)\n",
    "#                        if d.lower().startswith(\"epctl\") and os.path.isdir(os.path.join(data_dir, d))])\n",
    "\n",
    "# # Split 93 electrodes into 10 groups and select the first group.\n",
    "# electrode_indices = np.arange(93)\n",
    "# electrode_groups = np.array_split(electrode_indices, 10)\n",
    "# group = electrode_groups[8]\n",
    "\n",
    "# subject_batch = subject_dirs[24:25] \n",
    "# # subject_batch = subject_dirs[25:26]\n",
    "# # subject_batch = subject_dirs[26:27]\n",
    "\n",
    "# k = 2  # windows per electrode per subject (so k/2 for \"W\" and k/2 for \"N1\")\n",
    "# n_per_stage = k // 2\n",
    "# window_sec = 2   # window spans 2 sec (±1 sec)\n",
    "# half_window_sec = window_sec / 2\n",
    "\n",
    "# for subject in subject_batch:\n",
    "#     subj_path = os.path.join(data_dir, subject)\n",
    "#     csv_files = glob.glob(os.path.join(subj_path, \"*.csv\"))\n",
    "#     edf_files = glob.glob(os.path.join(subj_path, \"*.edf\"))\n",
    "#     if not csv_files or not edf_files:\n",
    "#         print(f\"Skipping {subject}: missing CSV or EDF.\")\n",
    "#         continue\n",
    "#     df = pd.read_csv(csv_files[0], index_col=0)\n",
    "#     df_W = df[df[\"stage\"]==\"W\"]\n",
    "#     df_N1 = df[df[\"stage\"]==\"N1\"]\n",
    "#     if len(df_W) < n_per_stage or len(df_N1) < n_per_stage:\n",
    "#         print(f\"Skipping {subject}: not enough epochs for one stage.\")\n",
    "#         continue\n",
    "#     # fixed random_state\n",
    "#     sample_W = df_W.sample(n=n_per_stage, random_state=3)[\"time_index\"].values\n",
    "#     sample_N1 = df_N1.sample(n=n_per_stage, random_state=3)[\"time_index\"].values\n",
    "\n",
    "#     reader = pyedflib.EdfReader(edf_files[0])\n",
    "#     fs = reader.getSampleFrequency(0)  # 1000 Hz\n",
    "#     total_samples = reader.getNSamples()[0]\n",
    "#     n_channels = reader.signals_in_file\n",
    "#     signals = [reader.readSignal(i) for i in range(n_channels)]\n",
    "#     channel_labels = reader.getSignalLabels()\n",
    "#     reader.close()\n",
    "\n",
    "#     subject_windows = {}\n",
    "#     for i in group:\n",
    "#         if i >= len(channel_labels):\n",
    "#             continue\n",
    "#         ch_label = channel_labels[i]\n",
    "#         for stage, sample_times in zip([\"W\", \"N1\"], [sample_W, sample_N1]):\n",
    "#             for j, t in enumerate(sample_times):\n",
    "#                 start = int((t - half_window_sec) * fs)\n",
    "#                 end = int((t + half_window_sec) * fs)\n",
    "#                 if start < 0 or end > total_samples:\n",
    "#                     continue\n",
    "#                 win = signals[i][start:end]\n",
    "#                 win = notch_filter(win, fs, freq=60, Q=30)\n",
    "#                 win = lowpass_filter(win, fs, cutoff=90, order=4)\n",
    "#                 ds_win = downsample_window(win, fs, target_fs=200)\n",
    "#                 key = f\"{subject.lower()}_{stage.lower()}_win_{j+1}_{ch_label.replace('-', '').replace(' ', '')}\"\n",
    "#                 # Save both the processed window and the center time index.\n",
    "#                 subject_windows[key] = {\"window\": ds_win, \"time_index\": t}\n",
    "\n",
    "#     out_path = os.path.join(subj_path, \"extracted_windows_group9.pkl\")\n",
    "#     with open(out_path, \"wb\") as f:\n",
    "#         pickle.dump(subject_windows, f)\n",
    "#     print(f\"Saved {len(subject_windows)} windows for {subject} (electrode group 9) at {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BATCH 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = \"/Users/tereza/spring_2025/STAT_4830/STAT-4830-GOALZ-project/data/ANPHY-Sleep_data\"\n",
    "# subject_dirs = sorted([d for d in os.listdir(data_dir)\n",
    "#                        if d.lower().startswith(\"epctl\") and os.path.isdir(os.path.join(data_dir, d))])\n",
    "\n",
    "# # Split 93 electrodes into 10 groups and select the first group.\n",
    "# electrode_indices = np.arange(93)\n",
    "# electrode_groups = np.array_split(electrode_indices, 10)\n",
    "# group = electrode_groups[9]\n",
    "\n",
    "# subject_batch = subject_dirs[27:28] \n",
    "# # subject_batch = subject_dirs[28:29]\n",
    "\n",
    "# k = 2  # windows per electrode per subject (so k/2 for \"W\" and k/2 for \"N1\")\n",
    "# n_per_stage = k // 2\n",
    "# window_sec = 2   # window spans 2 sec (±1 sec)\n",
    "# half_window_sec = window_sec / 2\n",
    "\n",
    "# for subject in subject_batch:\n",
    "#     subj_path = os.path.join(data_dir, subject)\n",
    "#     csv_files = glob.glob(os.path.join(subj_path, \"*.csv\"))\n",
    "#     edf_files = glob.glob(os.path.join(subj_path, \"*.edf\"))\n",
    "#     if not csv_files or not edf_files:\n",
    "#         print(f\"Skipping {subject}: missing CSV or EDF.\")\n",
    "#         continue\n",
    "#     df = pd.read_csv(csv_files[0], index_col=0)\n",
    "#     df_W = df[df[\"stage\"]==\"W\"]\n",
    "#     df_N1 = df[df[\"stage\"]==\"N1\"]\n",
    "#     if len(df_W) < n_per_stage or len(df_N1) < n_per_stage:\n",
    "#         print(f\"Skipping {subject}: not enough epochs for one stage.\")\n",
    "#         continue\n",
    "#     # fixed random_state\n",
    "#     sample_W = df_W.sample(n=n_per_stage, random_state=3)[\"time_index\"].values\n",
    "#     sample_N1 = df_N1.sample(n=n_per_stage, random_state=3)[\"time_index\"].values\n",
    "\n",
    "#     reader = pyedflib.EdfReader(edf_files[0])\n",
    "#     fs = reader.getSampleFrequency(0)  # 1000 Hz\n",
    "#     total_samples = reader.getNSamples()[0]\n",
    "#     n_channels = reader.signals_in_file\n",
    "#     signals = [reader.readSignal(i) for i in range(n_channels)]\n",
    "#     channel_labels = reader.getSignalLabels()\n",
    "#     reader.close()\n",
    "\n",
    "#     subject_windows = {}\n",
    "#     for i in group:\n",
    "#         if i >= len(channel_labels):\n",
    "#             continue\n",
    "#         ch_label = channel_labels[i]\n",
    "#         for stage, sample_times in zip([\"W\", \"N1\"], [sample_W, sample_N1]):\n",
    "#             for j, t in enumerate(sample_times):\n",
    "#                 start = int((t - half_window_sec) * fs)\n",
    "#                 end = int((t + half_window_sec) * fs)\n",
    "#                 if start < 0 or end > total_samples:\n",
    "#                     continue\n",
    "#                 win = signals[i][start:end]\n",
    "#                 win = notch_filter(win, fs, freq=60, Q=30)\n",
    "#                 win = lowpass_filter(win, fs, cutoff=90, order=4)\n",
    "#                 ds_win = downsample_window(win, fs, target_fs=200)\n",
    "#                 key = f\"{subject.lower()}_{stage.lower()}_win_{j+1}_{ch_label.replace('-', '').replace(' ', '')}\"\n",
    "#                 # Save both the processed window and the center time index.\n",
    "#                 subject_windows[key] = {\"window\": ds_win, \"time_index\": t}\n",
    "\n",
    "#     out_path = os.path.join(subj_path, \"extracted_windows_group10.pkl\")\n",
    "#     with open(out_path, \"wb\") as f:\n",
    "#         pickle.dump(subject_windows, f)\n",
    "#     print(f\"Saved {len(subject_windows)} windows for {subject} (electrode group 10) at {out_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnt_gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
