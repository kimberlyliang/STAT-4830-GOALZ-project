{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, pickle, numpy as np, pandas as pd, pyedflib\n",
    "from scipy.signal import butter, filtfilt, resample_poly, iirnotch\n",
    "import matplotlib.pyplot as plt\n",
    "from config.config import DataConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def notch_filter(data, fs, freq=60, Q=30):\n",
    "    b, a = iirnotch(freq, Q, fs)\n",
    "    return filtfilt(b, a, data)\n",
    "\n",
    "def lowpass_filter(data, fs, cutoff=90, order=4):\n",
    "    nyq = fs/2.0\n",
    "    normal_cutoff = cutoff/nyq\n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    return filtfilt(b, a, data)\n",
    "\n",
    "def downsample_window(data, fs, target_fs=200):\n",
    "    return resample_poly(data, target_fs, fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kimberly/Documents/STAT4830/STAT-4830-GOALZ-project/Anphy Dataset\n",
      "Subject folder: EPCTL06\n"
     ]
    }
   ],
   "source": [
    "data_dir = DataConfig.BASE_PATH\n",
    "print(data_dir)\n",
    "subject_dirs = sorted([d for d in os.listdir(data_dir)\n",
    "                       if d.lower().startswith(\"epctl\") and os.path.isdir(os.path.join(data_dir, d))])\n",
    "subject_batch = subject_dirs[5:6]\n",
    "# subject_batch = subject_dirs[0:1]\n",
    "# subject_batch = subject_dirs[1:2]\n",
    "for subject in subject_batch:\n",
    "    print(\"Subject folder:\", subject)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BATCH 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = DataConfig.BASE_PATH\n",
    "# subject_dirs = sorted([d for d in os.listdir(data_dir)\n",
    "#                        if d.lower().startswith(\"epctl\") and os.path.isdir(os.path.join(data_dir, d))])\n",
    "\n",
    "# # Split 93 electrodes into 10 groups and select the first group.\n",
    "# electrode_indices = np.arange(93)\n",
    "# electrode_groups = np.array_split(electrode_indices, 10)\n",
    "# first_group = electrode_groups[0]\n",
    "\n",
    "# # subject_batch = subject_dirs[0:1]\n",
    "# # subject_batch = subject_dirs[1:2]\n",
    "# # subject_batch = subject_dirs[2:3]\n",
    "# # subject_batch = subject_dirs[3:4]\n",
    "\n",
    "# k = 2  # windows per electrode per subject (so k/2 for \"W\" and k/2 for \"N1\")\n",
    "# n_per_stage = k // 2\n",
    "# window_sec = 2   # window spans 2 sec (±1 sec)\n",
    "# half_window_sec = window_sec / 2\n",
    "\n",
    "# for subject in subject_batch:\n",
    "#     subj_path = os.path.join(data_dir, subject)\n",
    "#     print(subj_path)\n",
    "#     csv_files = glob.glob(os.path.join(subj_path, \"*.csv\"))\n",
    "#     print(csv_files)\n",
    "#     edf_files = glob.glob(os.path.join(subj_path, \"*.edf\"))\n",
    "#     if not csv_files or not edf_files:\n",
    "#         print(f\"Skipping {subject}: missing CSV or EDF.\")\n",
    "#         continue\n",
    "#     df = pd.read_csv(csv_files[0], index_col=0) # had to change to 1 because that is what it is on mine\n",
    "#     df_W = df[df[\"stage\"]==\"W\"]\n",
    "#     df_N1 = df[df[\"stage\"]==\"N1\"]\n",
    "#     df_N2 = df[df[\"stage\"]==\"N2\"]\n",
    "#     df_N3 = df[df[\"stage\"]==\"N3\"]\n",
    "#     df_R = df[df[\"stage\"]==\"R\"]\n",
    "\n",
    "#     if len(df_W) < n_per_stage or len(df_N1) < n_per_stage:\n",
    "#         print(f\"Skipping {subject}: not enough epochs for one stage.\")\n",
    "#         continue\n",
    "#     # fixed random_state for reproducibility.\n",
    "#     sample_W = df_W.sample(n=n_per_stage, random_state=0)[\"time_index\"].values\n",
    "#     sample_N1 = df_N1.sample(n=n_per_stage, random_state=0)[\"time_index\"].values\n",
    "#     sample_N2 = df_N2.sample(n=n_per_stage, random_state=0)[\"time_index\"].values\n",
    "#     sample_N3 = df_N3.sample(n=n_per_stage, random_state=0)[\"time_index\"].values\n",
    "#     sample_R = df_R.sample(n=n_per_stage, random_state=0)[\"time_index\"].values\n",
    "\n",
    "#     reader = pyedflib.EdfReader(edf_files[0])\n",
    "#     fs = reader.getSampleFrequency(0)  # 1000 Hz\n",
    "#     total_samples = reader.getNSamples()[0]\n",
    "#     n_channels = reader.signals_in_file\n",
    "#     signals = [reader.readSignal(i) for i in range(n_channels)]\n",
    "#     channel_labels = reader.getSignalLabels()\n",
    "#     reader.close()\n",
    "\n",
    "#     subject_windows = {}\n",
    "#     # Process only electrodes in the first group.\n",
    "#     for i in first_group:\n",
    "#         if i >= len(channel_labels):\n",
    "#             continue\n",
    "#         ch_label = channel_labels[i]\n",
    "#         for stage, sample_times in zip([\"W\", \"N1\", \"N2\", \"N3\", \"R\"], [sample_W, sample_N1, sample_N1, sample_N2, sample_N3, sample_R]):\n",
    "#             for j, t in enumerate(sample_times):\n",
    "#                 start = int((t - half_window_sec) * fs)\n",
    "#                 end = int((t + half_window_sec) * fs)\n",
    "#                 if start < 0 or end > total_samples:\n",
    "#                     continue\n",
    "#                 win = signals[i][start:end]\n",
    "#                 win = notch_filter(win, fs, freq=60, Q=30)\n",
    "#                 win = lowpass_filter(win, fs, cutoff=90, order=4)\n",
    "#                 ds_win = downsample_window(win, fs, target_fs=200)\n",
    "#                 key = f\"{subject.lower()}_{stage.lower()}_win_{j+1}_{ch_label.replace('-', '').replace(' ', '')}\"\n",
    "#                 # Save both the processed window and the center time index.\n",
    "#                 subject_windows[key] = {\"window\": ds_win, \"time_index\": t}\n",
    "\n",
    "#     out_path = os.path.join(subj_path, \"complete_extracted_windows_group1.pkl\")\n",
    "#     with open(out_path, \"wb\") as f:\n",
    "#         pickle.dump(subject_windows, f)\n",
    "#     print(f\"Saved {len(subject_windows)} windows for {subject} (electrode group 1) at {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BATCH 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject_dirs = sorted([d for d in os.listdir(data_dir)\n",
    "#                        if d.lower().startswith(\"epctl\") and os.path.isdir(os.path.join(data_dir, d))])\n",
    "\n",
    "# # Split 93 electrodes into 10 groups and select the first group.\n",
    "# electrode_indices = np.arange(93)\n",
    "# electrode_groups = np.array_split(electrode_indices, 10)\n",
    "# # first_group = electrode_groups[0]\n",
    "# second_group = electrode_groups[1]\n",
    "\n",
    "# # subject_batch = subject_dirs[3:4] # sub 4 - done\n",
    "# # subject_batch = subject_dirs[4:5]\n",
    "# # subject_batch = subject_dirs[5:6]\n",
    "\n",
    "# k = 2  # windows per electrode per subject (so k/2 for \"W\" and k/2 for \"N1\")\n",
    "# n_per_stage = k // 2\n",
    "# window_sec = 2   # window spans 2 sec (±1 sec)\n",
    "# half_window_sec = window_sec / 2\n",
    "\n",
    "# for subject in subject_batch:\n",
    "#     subj_path = os.path.join(data_dir, subject)\n",
    "#     print(subj_path)\n",
    "#     csv_files = glob.glob(os.path.join(subj_path, \"*.csv\"))\n",
    "#     print(csv_files)\n",
    "#     edf_files = glob.glob(os.path.join(subj_path, \"*.edf\"))\n",
    "#     if not csv_files or not edf_files:\n",
    "#         print(f\"Skipping {subject}: missing CSV or EDF.\")\n",
    "#         continue\n",
    "#     df = pd.read_csv(csv_files[0], index_col=0)\n",
    "#     df_W = df[df[\"stage\"]==\"W\"]\n",
    "#     df_N1 = df[df[\"stage\"]==\"N1\"]\n",
    "#     df_N2 = df[df[\"stage\"]==\"N2\"]\n",
    "#     df_N3 = df[df[\"stage\"]==\"N3\"]\n",
    "#     df_R = df[df[\"stage\"]==\"R\"]\n",
    "    \n",
    "#     if len(df_W) < n_per_stage or len(df_N1) < n_per_stage:\n",
    "#         print(f\"Skipping {subject}: not enough epochs for one stage.\")\n",
    "#         continue\n",
    "#     # fixed random_state for reproducibility.\n",
    "#     sample_W = df_W.sample(n=n_per_stage, random_state=0)[\"time_index\"].values\n",
    "#     sample_N1 = df_N1.sample(n=n_per_stage, random_state=0)[\"time_index\"].values\n",
    "#     sample_N2 = df_N2.sample(n=n_per_stage, random_state=0)[\"time_index\"].values\n",
    "#     sample_N3 = df_N3.sample(n=n_per_stage, random_state=0)[\"time_index\"].values\n",
    "#     sample_R = df_R.sample(n=n_per_stage, random_state=0)[\"time_index\"].values\n",
    "\n",
    "#     reader = pyedflib.EdfReader(edf_files[0])\n",
    "#     fs = reader.getSampleFrequency(0)  # 1000 Hz\n",
    "#     total_samples = reader.getNSamples()[0]\n",
    "#     n_channels = reader.signals_in_file\n",
    "#     signals = [reader.readSignal(i) for i in range(n_channels)]\n",
    "#     channel_labels = reader.getSignalLabels()\n",
    "#     reader.close()\n",
    "\n",
    "#     subject_windows = {}\n",
    "#     # Process only electrodes in the first group.\n",
    "#     for i in second_group:\n",
    "#         if i >= len(channel_labels):\n",
    "#             continue\n",
    "#         ch_label = channel_labels[i]\n",
    "#         for stage, sample_times in zip([\"W\", \"N1\", \"N2\", \"N3\", \"R\"], [sample_W, sample_N1, sample_N1, sample_N2, sample_N3, sample_R]):\n",
    "#             for j, t in enumerate(sample_times):\n",
    "#                 start = int((t - half_window_sec) * fs)\n",
    "#                 end = int((t + half_window_sec) * fs)\n",
    "#                 if start < 0 or end > total_samples:\n",
    "#                     continue\n",
    "#                 win = signals[i][start:end]\n",
    "#                 win = notch_filter(win, fs, freq=60, Q=30)\n",
    "#                 win = lowpass_filter(win, fs, cutoff=90, order=4)\n",
    "#                 ds_win = downsample_window(win, fs, target_fs=200)\n",
    "#                 key = f\"{subject.lower()}_{stage.lower()}_win_{j+1}_{ch_label.replace('-', '').replace(' ', '')}\"\n",
    "#                 # Save both the processed window and the center time index.\n",
    "#                 subject_windows[key] = {\"window\": ds_win, \"time_index\": t}\n",
    "\n",
    "#     out_path = os.path.join(subj_path, \"complete_extracted_windows_group2.pkl\")\n",
    "#     with open(out_path, \"wb\") as f:\n",
    "#         pickle.dump(subject_windows, f)\n",
    "#     print(f\"Saved {len(subject_windows)} windows for {subject} (electrode group 2) at {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BATCH 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject_dirs = sorted([d for d in os.listdir(data_dir)\n",
    "#                        if d.lower().startswith(\"epctl\") and os.path.isdir(os.path.join(data_dir, d))])\n",
    "\n",
    "# # Split 93 electrodes into 10 groups and select the first group.\n",
    "# electrode_indices = np.arange(93)\n",
    "# electrode_groups = np.array_split(electrode_indices, 10)\n",
    "# # first_group = electrode_groups[0]\n",
    "# group = electrode_groups[2]\n",
    "\n",
    "# # subject_batch = subject_dirs[6:7] \n",
    "# # subject_batch = subject_dirs[7:8]\n",
    "# # subject_batch = subject_dirs[8:9]\n",
    "\n",
    "# k = 2  # windows per electrode per subject (so k/2 for \"W\" and k/2 for \"N1\")\n",
    "# n_per_stage = k // 2\n",
    "# window_sec = 2   # window spans 2 sec (±1 sec)\n",
    "# half_window_sec = window_sec / 2\n",
    "\n",
    "# for subject in subject_batch:\n",
    "#     subj_path = os.path.join(data_dir, subject)\n",
    "#     csv_files = glob.glob(os.path.join(subj_path, \"*.csv\"))\n",
    "#     edf_files = glob.glob(os.path.join(subj_path, \"*.edf\"))\n",
    "#     if not csv_files or not edf_files:\n",
    "#         print(f\"Skipping {subject}: missing CSV or EDF.\")\n",
    "#         continue\n",
    "#     df = pd.read_csv(csv_files[1], index_col=0)\n",
    "#     df_W = df[df[\"stage\"]==\"W\"]\n",
    "#     df_N1 = df[df[\"stage\"]==\"N1\"]\n",
    "#     df_N2 = df[df[\"stage\"]==\"N2\"]\n",
    "#     df_N3 = df[df[\"stage\"]==\"N3\"]\n",
    "#     df_R = df[df[\"stage\"]==\"R\"]\n",
    "#     if len(df_W) < n_per_stage or len(df_N1) < n_per_stage:\n",
    "#         print(f\"Skipping {subject}: not enough epochs for one stage.\")\n",
    "#         continue\n",
    "#     sample_W = df_W.sample(n=n_per_stage, random_state=0)[\"time_index\"].values\n",
    "#     sample_N1 = df_N1.sample(n=n_per_stage, random_state=0)[\"time_index\"].values\n",
    "#     sample_N2 = df_N2.sample(n=n_per_stage, random_state=0)[\"time_index\"].values\n",
    "#     sample_N3 = df_N3.sample(n=n_per_stage, random_state=0)[\"time_index\"].values\n",
    "#     sample_R = df_R.sample(n=n_per_stage, random_state=0)[\"time_index\"].values\n",
    "\n",
    "#     reader = pyedflib.EdfReader(edf_files[0])\n",
    "#     fs = reader.getSampleFrequency(0)  # 1000 Hz\n",
    "#     total_samples = reader.getNSamples()[0]\n",
    "#     n_channels = reader.signals_in_file\n",
    "#     signals = [reader.readSignal(i) for i in range(n_channels)]\n",
    "#     channel_labels = reader.getSignalLabels()\n",
    "#     reader.close()\n",
    "\n",
    "#     subject_windows = {}\n",
    "#     # Process only electrodes in the first group.\n",
    "#     for i in group:\n",
    "#         if i >= len(channel_labels):\n",
    "#             continue\n",
    "#         ch_label = channel_labels[i]\n",
    "#         for stage, sample_times in zip([\"W\", \"N1\", \"N2\", \"N3\", \"R\"], [sample_W, sample_N1, sample_N1, sample_N2, sample_N3, sample_R]):\n",
    "#             for j, t in enumerate(sample_times):\n",
    "#                 start = int((t - half_window_sec) * fs)\n",
    "#                 end = int((t + half_window_sec) * fs)\n",
    "#                 if start < 0 or end > total_samples:\n",
    "#                     continue\n",
    "#                 win = signals[i][start:end]\n",
    "#                 win = notch_filter(win, fs, freq=60, Q=30)\n",
    "#                 win = lowpass_filter(win, fs, cutoff=90, order=4)\n",
    "#                 ds_win = downsample_window(win, fs, target_fs=200)\n",
    "#                 key = f\"{subject.lower()}_{stage.lower()}_win_{j+1}_{ch_label.replace('-', '').replace(' ', '')}\"\n",
    "#                 # Save both the processed window and the center time index.\n",
    "#                 subject_windows[key] = {\"window\": ds_win, \"time_index\": t}\n",
    "\n",
    "#     out_path = os.path.join(subj_path, \"complete_extracted_windows_group3.pkl\")\n",
    "#     with open(out_path, \"wb\") as f:\n",
    "#         pickle.dump(subject_windows, f)\n",
    "#     print(f\"Saved {len(subject_windows)} windows for {subject} (electrode group 3) at {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BATCH 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject_dirs = sorted([d for d in os.listdir(data_dir)\n",
    "#                        if d.lower().startswith(\"epctl\") and os.path.isdir(os.path.join(data_dir, d))])\n",
    "\n",
    "# # Split 93 electrodes into 10 groups and select the first group.\n",
    "# electrode_indices = np.arange(93)\n",
    "# electrode_groups = np.array_split(electrode_indices, 10)\n",
    "# # first_group = electrode_groups[0]\n",
    "# group = electrode_groups[3]\n",
    "\n",
    "# # subject_batch = subject_dirs[9:10] # done\n",
    "# # subject_batch = subject_dirs[10:11]\n",
    "# subject_batch = subject_dirs[11:12]\n",
    "\n",
    "# k = 2  # windows per electrode per subject (so k/2 for \"W\" and k/2 for \"N1\")\n",
    "# n_per_stage = k // 2\n",
    "# window_sec = 2   # window spans 2 sec (±1 sec)\n",
    "# half_window_sec = window_sec / 2\n",
    "\n",
    "# for subject in subject_batch:\n",
    "#     subj_path = os.path.join(data_dir, subject)\n",
    "#     csv_files = glob.glob(os.path.join(subj_path, \"*.csv\"))\n",
    "#     edf_files = glob.glob(os.path.join(subj_path, \"*.edf\"))\n",
    "#     if not csv_files or not edf_files:\n",
    "#         print(f\"Skipping {subject}: missing CSV or EDF.\")\n",
    "#         continue\n",
    "#     df = pd.read_csv(csv_files[1], index_col=0)\n",
    "#     df_W = df[df[\"stage\"]==\"W\"]\n",
    "#     df_N1 = df[df[\"stage\"]==\"N1\"]\n",
    "#     df_N2 = df[df[\"stage\"]==\"N2\"]\n",
    "#     df_N3 = df[df[\"stage\"]==\"N3\"]\n",
    "#     df_R = df[df[\"stage\"]==\"R\"]\n",
    "#     if len(df_W) < n_per_stage or len(df_N1) < n_per_stage:\n",
    "#         print(f\"Skipping {subject}: not enough epochs for one stage.\")\n",
    "#         continue\n",
    "#     # fixed random_state for reproducibility.\n",
    "#     sample_W = df_W.sample(n=n_per_stage, random_state=0)[\"time_index\"].values\n",
    "#     sample_N1 = df_N1.sample(n=n_per_stage, random_state=0)[\"time_index\"].values\n",
    "#     sample_N2 = df_N2.sample(n=n_per_stage, random_state=0)[\"time_index\"].values\n",
    "#     sample_N3 = df_N3.sample(n=n_per_stage, random_state=0)[\"time_index\"].values\n",
    "#     sample_R = df_R.sample(n=n_per_stage, random_state=0)[\"time_index\"].values\n",
    "\n",
    "#     reader = pyedflib.EdfReader(edf_files[0])\n",
    "#     fs = reader.getSampleFrequency(0)  # 1000 Hz\n",
    "#     total_samples = reader.getNSamples()[0]\n",
    "#     n_channels = reader.signals_in_file\n",
    "#     signals = [reader.readSignal(i) for i in range(n_channels)]\n",
    "#     channel_labels = reader.getSignalLabels()\n",
    "#     reader.close()\n",
    "\n",
    "#     subject_windows = {}\n",
    "#     for i in group:\n",
    "#         if i >= len(channel_labels):\n",
    "#             continue\n",
    "#         ch_label = channel_labels[i]\n",
    "#         for stage, sample_times in zip([\"W\", \"N1\", \"N2\", \"N3\", \"R\"], [sample_W, sample_N1, sample_N1, sample_N2, sample_N3, sample_R]):\n",
    "#             for j, t in enumerate(sample_times):\n",
    "#                 start = int((t - half_window_sec) * fs)\n",
    "#                 end = int((t + half_window_sec) * fs)\n",
    "#                 if start < 0 or end > total_samples:\n",
    "#                     continue\n",
    "#                 win = signals[i][start:end]\n",
    "#                 win = notch_filter(win, fs, freq=60, Q=30)\n",
    "#                 win = lowpass_filter(win, fs, cutoff=90, order=4)\n",
    "#                 ds_win = downsample_window(win, fs, target_fs=200)\n",
    "#                 key = f\"{subject.lower()}_{stage.lower()}_win_{j+1}_{ch_label.replace('-', '').replace(' ', '')}\"\n",
    "#                 # Save both the processed window and the center time index.\n",
    "#                 subject_windows[key] = {\"window\": ds_win, \"time_index\": t}\n",
    "\n",
    "#     out_path = os.path.join(subj_path, \"complete_extracted_windows_group4.pkl\")\n",
    "#     with open(out_path, \"wb\") as f:\n",
    "#         pickle.dump(subject_windows, f)\n",
    "#     print(f\"Saved {len(subject_windows)} windows for {subject} (electrode group 4) at {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BATCH 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject_dirs = sorted([d for d in os.listdir(data_dir)\n",
    "#                        if d.lower().startswith(\"epctl\") and os.path.isdir(os.path.join(data_dir, d))])\n",
    "\n",
    "# # Split 93 electrodes into 10 groups and select the first group.\n",
    "# electrode_indices = np.arange(93)\n",
    "# electrode_groups = np.array_split(electrode_indices, 10)\n",
    "# # first_group = electrode_groups[0]\n",
    "# group = electrode_groups[4]\n",
    "\n",
    "# # subject_batch = subject_dirs[12:13] # done\n",
    "# # subject_batch = subject_dirs[13:14] # done \n",
    "# # subject_batch = subject_dirs[14:15] # done\n",
    "\n",
    "# k = 2  # windows per electrode per subject (so k/2 for \"W\" and k/2 for \"N1\")\n",
    "# n_per_stage = k // 2\n",
    "# window_sec = 2   # window spans 2 sec (±1 sec)\n",
    "# half_window_sec = window_sec / 2\n",
    "\n",
    "# for subject in subject_batch:\n",
    "#     subj_path = os.path.join(data_dir, subject)\n",
    "#     csv_files = glob.glob(os.path.join(subj_path, \"*.csv\"))\n",
    "#     edf_files = glob.glob(os.path.join(subj_path, \"*.edf\"))\n",
    "#     if not csv_files or not edf_files:\n",
    "#         print(f\"Skipping {subject}: missing CSV or EDF.\")\n",
    "#         continue\n",
    "#     df = pd.read_csv(csv_files[0], index_col=0)\n",
    "#     df_W = df[df[\"stage\"]==\"W\"]\n",
    "#     df_N1 = df[df[\"stage\"]==\"N1\"]\n",
    "#     df_N2 = df[df[\"stage\"]==\"N2\"]\n",
    "#     df_N3 = df[df[\"stage\"]==\"N3\"]\n",
    "#     df_R = df[df[\"stage\"]==\"R\"]\n",
    "#     if len(df_W) < n_per_stage or len(df_N1) < n_per_stage:\n",
    "#         print(f\"Skipping {subject}: not enough epochs for one stage.\")\n",
    "#         continue\n",
    "#     # fixed random_state\n",
    "#     sample_W = df_W.sample(n=n_per_stage, random_state=0)[\"time_index\"].values\n",
    "#     sample_N1 = df_N1.sample(n=n_per_stage, random_state=0)[\"time_index\"].values\n",
    "#     sample_N2 = df_N2.sample(n=n_per_stage, random_state=0)[\"time_index\"].values\n",
    "#     sample_N3 = df_N3.sample(n=n_per_stage, random_state=0)[\"time_index\"].values\n",
    "#     sample_R = df_R.sample(n=n_per_stage, random_state=0)[\"time_index\"].values\n",
    "\n",
    "#     reader = pyedflib.EdfReader(edf_files[0])\n",
    "#     fs = reader.getSampleFrequency(0)  # 1000 Hz\n",
    "#     total_samples = reader.getNSamples()[0]\n",
    "#     n_channels = reader.signals_in_file\n",
    "#     signals = [reader.readSignal(i) for i in range(n_channels)]\n",
    "#     channel_labels = reader.getSignalLabels()\n",
    "#     reader.close()\n",
    "\n",
    "#     subject_windows = {}\n",
    "#     for i in group:\n",
    "#         if i >= len(channel_labels):\n",
    "#             continue\n",
    "#         ch_label = channel_labels[i]\n",
    "#         for stage, sample_times in zip([\"W\", \"N1\", \"N2\", \"N3\", \"R\"], [sample_W, sample_N1, sample_N1, sample_N2, sample_N3, sample_R]):\n",
    "#             for j, t in enumerate(sample_times):\n",
    "#                 start = int((t - half_window_sec) * fs)\n",
    "#                 end = int((t + half_window_sec) * fs)\n",
    "#                 if start < 0 or end > total_samples:\n",
    "#                     continue\n",
    "#                 win = signals[i][start:end]\n",
    "#                 win = notch_filter(win, fs, freq=60, Q=30)\n",
    "#                 win = lowpass_filter(win, fs, cutoff=90, order=4)\n",
    "#                 ds_win = downsample_window(win, fs, target_fs=200)\n",
    "#                 key = f\"{subject.lower()}_{stage.lower()}_win_{j+1}_{ch_label.replace('-', '').replace(' ', '')}\"\n",
    "#                 # Save both the processed window and the center time index.\n",
    "#                 subject_windows[key] = {\"window\": ds_win, \"time_index\": t}\n",
    "\n",
    "#     out_path = os.path.join(subj_path, \"complete_extracted_windows_group5.pkl\")\n",
    "#     with open(out_path, \"wb\") as f:\n",
    "#         pickle.dump(subject_windows, f)\n",
    "#     print(f\"Saved {len(subject_windows)} windows for {subject} (electrode group 5) at {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subj_path = \"/Users/tereza/spring_2025/STAT_4830/STAT-4830-GOALZ-project/data/ANPHY-Sleep_data/epctl01\"  \n",
    "\n",
    "# pkl_path = os.path.join(subj_path, \"extracted_windows_group1.pkl\")\n",
    "\n",
    "# with open(pkl_path, \"rb\") as f:\n",
    "#     windows_dict = pickle.load(f)\n",
    "\n",
    "# print(\"Total number of windows saved:\", len(windows_dict))\n",
    "# print(\"\\nSample entries:\")\n",
    "# for key in list(windows_dict.keys())[:5]:\n",
    "#     data = windows_dict[key]\n",
    "#     print(f\"Key: {key}\")\n",
    "#     print(\"  Window shape:\", data[\"window\"].shape)\n",
    "#     print(\"  Center time index:\", data[\"time_index\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BATCH 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject_dirs = sorted([d for d in os.listdir(data_dir)\n",
    "#                        if d.lower().startswith(\"epctl\") and os.path.isdir(os.path.join(data_dir, d))])\n",
    "\n",
    "# # Split 93 electrodes into 10 groups and select the first group.\n",
    "# electrode_indices = np.arange(93)\n",
    "# electrode_groups = np.array_split(electrode_indices, 10)\n",
    "# # first_group = electrode_groups[0]\n",
    "# group = electrode_groups[5]\n",
    "\n",
    "# # subject_batch = subject_dirs[15:16] \n",
    "# # subject_batch = subject_dirs[16:17] # done\n",
    "# # subject_batch = subject_dirs[17:18] # done\n",
    "\n",
    "# k = 2  # windows per electrode per subject (so k/2 for \"W\" and k/2 for \"N1\")\n",
    "# n_per_stage = k // 2\n",
    "# window_sec = 2   # window spans 2 sec (±1 sec)\n",
    "# half_window_sec = window_sec / 2\n",
    "\n",
    "# for subject in subject_batch:\n",
    "#     subj_path = os.path.join(data_dir, subject)\n",
    "#     csv_files = glob.glob(os.path.join(subj_path, \"*.csv\"))\n",
    "#     edf_files = glob.glob(os.path.join(subj_path, \"*.edf\"))\n",
    "#     if not csv_files or not edf_files:\n",
    "#         print(f\"Skipping {subject}: missing CSV or EDF.\")\n",
    "#         continue\n",
    "#     df = pd.read_csv(csv_files[1], index_col=0)\n",
    "#     df_W = df[df[\"stage\"]==\"W\"]\n",
    "#     df_N1 = df[df[\"stage\"]==\"N1\"]\n",
    "#     df_N2 = df[df[\"stage\"]==\"N2\"]\n",
    "#     df_N3 = df[df[\"stage\"]==\"N3\"]\n",
    "#     df_R = df[df[\"stage\"]==\"R\"]\n",
    "#     if len(df_W) < n_per_stage or len(df_N1) < n_per_stage:\n",
    "#         print(f\"Skipping {subject}: not enough epochs for one stage.\")\n",
    "#         continue\n",
    "#     # fixed random_state\n",
    "#     sample_W = df_W.sample(n=n_per_stage, random_state=0)[\"time_index\"].values\n",
    "#     sample_N1 = df_N1.sample(n=n_per_stage, random_state=0)[\"time_index\"].values\n",
    "#     sample_N2 = df_N2.sample(n=n_per_stage, random_state=0)[\"time_index\"].values\n",
    "#     sample_N3 = df_N3.sample(n=n_per_stage, random_state=0)[\"time_index\"].values\n",
    "#     sample_R = df_R.sample(n=n_per_stage, random_state=0)[\"time_index\"].values\n",
    "\n",
    "#     reader = pyedflib.EdfReader(edf_files[0])\n",
    "#     fs = reader.getSampleFrequency(0)  # 1000 Hz\n",
    "#     total_samples = reader.getNSamples()[0]\n",
    "#     n_channels = reader.signals_in_file\n",
    "#     signals = [reader.readSignal(i) for i in range(n_channels)]\n",
    "#     channel_labels = reader.getSignalLabels()\n",
    "#     reader.close()\n",
    "\n",
    "#     subject_windows = {}\n",
    "#     for i in group:\n",
    "#         if i >= len(channel_labels):\n",
    "#             continue\n",
    "#         ch_label = channel_labels[i]\n",
    "#         for stage, sample_times in zip([\"W\", \"N1\", \"N2\", \"N3\", \"R\"], [sample_W, sample_N1, sample_N1, sample_N2, sample_N3, sample_R]):\n",
    "#             for j, t in enumerate(sample_times):\n",
    "#                 start = int((t - half_window_sec) * fs)\n",
    "#                 end = int((t + half_window_sec) * fs)\n",
    "#                 if start < 0 or end > total_samples:\n",
    "#                     continue\n",
    "#                 win = signals[i][start:end]\n",
    "#                 win = notch_filter(win, fs, freq=60, Q=30)\n",
    "#                 win = lowpass_filter(win, fs, cutoff=90, order=4)\n",
    "#                 ds_win = downsample_window(win, fs, target_fs=200)\n",
    "#                 key = f\"{subject.lower()}_{stage.lower()}_win_{j+1}_{ch_label.replace('-', '').replace(' ', '')}\"\n",
    "#                 # Save both the processed window and the center time index.\n",
    "#                 subject_windows[key] = {\"window\": ds_win, \"time_index\": t}\n",
    "\n",
    "#     out_path = os.path.join(subj_path, \"complete_extracted_windows_group6.pkl\")\n",
    "#     with open(out_path, \"wb\") as f:\n",
    "#         pickle.dump(subject_windows, f)\n",
    "#     print(f\"Saved {len(subject_windows)} windows for {subject} (electrode group 6) at {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: The directory Anphy Dataset/EPCTL06 was not found.\n",
      "CSV files found: []\n",
      "EDF files found: []\n",
      "Skipping Anphy Dataset/EPCTL06: missing CSV or EDF\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Specify the path to your directory\n",
    "subj_path = \"Anphy Dataset/EPCTL06\"  # Adjust the path if necessary\n",
    "\n",
    "# List all files in the directory\n",
    "try:\n",
    "    files = os.listdir(subj_path)\n",
    "    print(\"Files in directory:\", files)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The directory {subj_path} was not found.\")\n",
    "\n",
    "# Check for CSV and EDF files\n",
    "csv_files = glob.glob(os.path.join(subj_path, \"*.csv\"))\n",
    "edf_files = glob.glob(os.path.join(subj_path, \"*.edf\"))\n",
    "\n",
    "# Print the found files\n",
    "print(\"CSV files found:\", csv_files)\n",
    "print(\"EDF files found:\", edf_files)\n",
    "\n",
    "# Check if any files were found\n",
    "if not csv_files and not edf_files:\n",
    "    print(f\"Skipping {subj_path}: missing CSV or EDF\")\n",
    "else:\n",
    "    print(f\"Found files in {subj_path}. Proceeding with processing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLOTTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subj_path = \"/Users/tereza/spring_2025/STAT_4830/STAT-4830-GOALZ-project/data/ANPHY-Sleep_data/epctl18\"\n",
    "# pkl_path = os.path.join(subj_path, \"extracted_windows_group6.pkl\")\n",
    "\n",
    "# with open(pkl_path, \"rb\") as f:\n",
    "#     windows_dict = pickle.load(f)\n",
    "\n",
    "# # Print all keys to verify available electrodes\n",
    "# all_keys = list(windows_dict.keys())\n",
    "# print(\"All keys in pkl:\", all_keys)\n",
    "\n",
    "# # For example, search for keys containing \"p1ref\" (not \"fp1ref\")\n",
    "# w_keys = [key for key in all_keys if \"p1ref\" in key.lower() and \"_w_\" in key.lower()]\n",
    "# n1_keys = [key for key in all_keys if \"p1ref\" in key.lower() and \"_n1_\" in key.lower()]\n",
    "\n",
    "# print(\"W keys for p1ref:\", w_keys)\n",
    "# print(\"N1 keys for p1ref:\", n1_keys)\n",
    "\n",
    "# if w_keys and n1_keys:\n",
    "#     window_w = windows_dict[w_keys[0]][\"window\"]\n",
    "#     window_n1 = windows_dict[n1_keys[0]][\"window\"]\n",
    "    \n",
    "#     plt.figure(figsize=(10, 5))\n",
    "#     plt.plot(window_w, label=\"Wake (W)\")\n",
    "#     plt.plot(window_n1, label=\"N1\")\n",
    "#     plt.xlabel(\"Sample Index (at 200 Hz)\")\n",
    "#     plt.ylabel(\"Amplitude\")\n",
    "#     plt.title(\"2-second Windows for electrode P1Ref\")\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "# else:\n",
    "#     print(\"Could not find both W and N1 windows for electrode P1Ref\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Anphy Dataset/EPCTL06/complete_extracted_windows_group6.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m pkl_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(subj_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomplete_extracted_windows_group6.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpkl_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      4\u001b[0m     windows_dict \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Get unique electrode labels from the keys.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.13/lib/python/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Anphy Dataset/EPCTL06/complete_extracted_windows_group6.pkl'"
     ]
    }
   ],
   "source": [
    "subj_path = os.path.join(DataConfig.BASE_PATH, \"/EPCTL06\")\n",
    "pkl_path = os.path.join(subj_path, \"complete_extracted_windows_group6.pkl\")\n",
    "\n",
    "with open(pkl_path, \"rb\") as f:\n",
    "    windows_dict = pickle.load(f)\n",
    "\n",
    "# Get unique electrode labels from the keys.\n",
    "unique_electrodes = set()\n",
    "for key in windows_dict.keys():\n",
    "    # Assuming the electrode label is the last part of the key separated by underscores.\n",
    "    elec_label = key.split(\"_\")[-1]\n",
    "    unique_electrodes.add(elec_label)\n",
    "\n",
    "unique_electrodes = sorted(unique_electrodes)\n",
    "print(\"Unique electrode labels:\", unique_electrodes)\n",
    "\n",
    "# For each electrode, find the corresponding W and N1 keys and plot their windows.\n",
    "for elec in unique_electrodes:\n",
    "    # Find keys that contain the electrode label and stage.\n",
    "    w_keys = [k for k in windows_dict if elec.lower() in k.lower() and \"_w_\" in k.lower()]\n",
    "    n1_keys = [k for k in windows_dict if elec.lower() in k.lower() and \"_n1_\" in k.lower()]\n",
    "    n2_keys = [k for k in windows_dict if elec.lower() in k.lower() and \"_n2_\" in k.lower()]\n",
    "    n3_keys = [k for k in windows_dict if elec.lower() in k.lower() and \"_n3_\" in k.lower()]\n",
    "    r_keys = [k for k in windows_dict if elec.lower() in k.lower() and \"_r_\" in k.lower()]\n",
    "\n",
    "        # Check if all keys are found\n",
    "    if w_keys and n1_keys and n2_keys and n3_keys and r_keys:\n",
    "        window_w = windows_dict[w_keys[0]][\"window\"]\n",
    "        window_n1 = windows_dict[n1_keys[0]][\"window\"]\n",
    "        window_n2 = windows_dict[n2_keys[0]][\"window\"]\n",
    "        window_n3 = windows_dict[n3_keys[0]][\"window\"]\n",
    "        window_r = windows_dict[r_keys[0]][\"window\"]\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(window_w, marker='o', linestyle='-', label=\"Wake (W)\")\n",
    "        plt.plot(window_n1, marker='o', linestyle='-', label=\"N1\")\n",
    "        plt.plot(window_n2, marker='o', linestyle='-', label=\"N2\")\n",
    "        plt.plot(window_n3, marker='o', linestyle='-', label=\"N3\")\n",
    "        plt.plot(window_r, marker='o', linestyle='-', label=\"R\")\n",
    "        plt.xlabel(\"Sample Index (at 200 Hz)\")\n",
    "        plt.ylabel(\"Amplitude\")\n",
    "        plt.title(f\"2-second Windows for electrode {elec}\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Missing windows for electrode: {elec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/tereza/spring_2025/STAT_4830/STAT-4830-GOALZ-project/data/ANPHY-Sleep_data/epctl18/extracted_windows_group6.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m subj_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/tereza/spring_2025/STAT_4830/STAT-4830-GOALZ-project/data/ANPHY-Sleep_data/epctl18\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m pkl_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(subj_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextracted_windows_group6.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpkl_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      5\u001b[0m     windows_dict \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Collect unique electrode labels (last underscore-separated part of each key).\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.13/lib/python/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/tereza/spring_2025/STAT_4830/STAT-4830-GOALZ-project/data/ANPHY-Sleep_data/epctl18/extracted_windows_group6.pkl'"
     ]
    }
   ],
   "source": [
    "# subj_path = \"/Users/tereza/spring_2025/STAT_4830/STAT-4830-GOALZ-project/data/ANPHY-Sleep_data/epctl18\"\n",
    "# pkl_path = os.path.join(subj_path, \"extracted_windows_group6.pkl\")\n",
    "\n",
    "# with open(pkl_path, \"rb\") as f:\n",
    "#     windows_dict = pickle.load(f)\n",
    "\n",
    "# # Collect unique electrode labels (last underscore-separated part of each key).\n",
    "# unique_electrodes = set()\n",
    "# for key in windows_dict.keys():\n",
    "#     elec_label = key.split(\"_\")[-1]\n",
    "#     unique_electrodes.add(elec_label)\n",
    "\n",
    "# unique_electrodes = sorted(unique_electrodes)\n",
    "\n",
    "# for elec in unique_electrodes:\n",
    "#     # Find the key for W\n",
    "#     w_keys = [k for k in windows_dict if elec.lower() in k.lower() and \"_w_\" in k.lower()]\n",
    "#     # Find the key for N1\n",
    "#     n1_keys = [k for k in windows_dict if elec.lower() in k.lower() and \"_n1_\" in k.lower()]\n",
    "\n",
    "#     n2_keys = [k for k in windows_dict if elec.lower() in k.lower() and \"_n2_\" in k.lower()]\n",
    "\n",
    "#     n3_keys = [k for k in windows_dict if elec.lower() in k.lower() and \"_n3_\" in k.lower()]\n",
    "\n",
    "#     r_keys = [k for k in windows_dict if elec.lower() in k.lower() and \"_r_\" in k.lower()]\n",
    "\n",
    "#     if w_keys and n1_keys and n2_keys and n3_keys and r_keys:\n",
    "#         w_key = w_keys[0]\n",
    "#         n1_key = n1_keys[0]\n",
    "#         n2_key = n2_keys[0]\n",
    "#         n3_key = n3_keys[0]\n",
    "#         r_key = r_keys[0]\n",
    "#         w_time = windows_dict[w_key][\"time_index\"]\n",
    "#         n1_time = windows_dict[n1_key][\"time_index\"]\n",
    "#         n2_time = windows_dict[n2_key][\"time_index\"]\n",
    "#         n3_time = windows_dict[n3_key][\"time_index\"]\n",
    "#         r_time = windows_dict[r_key][\"time_index\"]\n",
    "#         print(f\"Electrode: {elec}\")\n",
    "#         print(f\"  Wake clip centered at t={w_time:.2f} s (key: {w_key})\")\n",
    "#         print(f\"  N1 clip centered at t={n1_time:.2f} s (key: {n1_key})\")\n",
    "        \n",
    "#     else:\n",
    "#         print(f\"Electrode: {elec} has incomplete data (missing W or N1).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BATCH 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m subject_dirs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m([d \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mlistdir(data_dir)\n\u001b[1;32m      2\u001b[0m                        \u001b[38;5;28;01mif\u001b[39;00m d\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepctl\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, d))])\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Split 93 electrodes into 10 groups and select the first group.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m electrode_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m93\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "subject_dirs = sorted([d for d in os.listdir(data_dir)\n",
    "                       if d.lower().startswith(\"epctl\") and os.path.isdir(os.path.join(data_dir, d))])\n",
    "\n",
    "# Split 93 electrodes into 10 groups and select the first group.\n",
    "electrode_indices = np.arange(93)\n",
    "electrode_groups = np.array_split(electrode_indices, 10)\n",
    "# first_group = electrode_groups[0]\n",
    "group = electrode_groups[6]\n",
    "\n",
    "# subject_batch = subject_dirs[18:19] # 12:12 done\n",
    "# subject_batch = subject_dirs[19:20] # done\n",
    "subject_batch = subject_dirs[20:21] # done\n",
    "\n",
    "k = 2  # windows per electrode per subject (so k/2 for \"W\" and k/2 for \"N1\")\n",
    "n_per_stage = k // 2\n",
    "window_sec = 2   # window spans 2 sec (±1 sec)\n",
    "half_window_sec = window_sec / 2\n",
    "\n",
    "for subject in subject_batch:\n",
    "    subj_path = os.path.join(data_dir, subject)\n",
    "    csv_files = glob.glob(os.path.join(subj_path, \"*.csv\"))\n",
    "    edf_files = glob.glob(os.path.join(subj_path, \"*.edf\"))\n",
    "    if not csv_files or not edf_files:\n",
    "        print(f\"Skipping {subject}: missing CSV or EDF.\")\n",
    "        continue\n",
    "    df = pd.read_csv(csv_files[0], index_col=0)\n",
    "    df_W = df[df[\"stage\"]==\"W\"]\n",
    "    df_N1 = df[df[\"stage\"]==\"N1\"]\n",
    "    df_N2 = df[df[\"stage\"]==\"N2\"]\n",
    "    df_N3 = df[df[\"stage\"]==\"N3\"]\n",
    "    df_R = df[df[\"stage\"]==\"R\"]\n",
    "    if len(df_W) < n_per_stage or len(df_N1) < n_per_stage:\n",
    "        print(f\"Skipping {subject}: not enough epochs for one stage.\")\n",
    "        continue\n",
    "    # fixed random_state\n",
    "    sample_W = df_W.sample(n=n_per_stage, random_state=0)[\"time_index\"].values\n",
    "    sample_N1 = df_N1.sample(n=n_per_stage, random_state=0)[\"time_index\"].values\n",
    "    sample_N2 = df_N2.sample(n=n_per_stage, random_state=0)[\"time_index\"].values\n",
    "    sample_N3 = df_N3.sample(n=n_per_stage, random_state=0)[\"time_index\"].values\n",
    "    sample_R = df_R.sample(n=n_per_stage, random_state=0)[\"time_index\"].values\n",
    "\n",
    "    reader = pyedflib.EdfReader(edf_files[0])\n",
    "    fs = reader.getSampleFrequency(0)  # 1000 Hz\n",
    "    total_samples = reader.getNSamples()[0]\n",
    "    n_channels = reader.signals_in_file\n",
    "    signals = [reader.readSignal(i) for i in range(n_channels)]\n",
    "    channel_labels = reader.getSignalLabels()\n",
    "    reader.close()\n",
    "\n",
    "    subject_windows = {}\n",
    "    for i in group:\n",
    "        if i >= len(channel_labels):\n",
    "            continue\n",
    "        ch_label = channel_labels[i]\n",
    "        for stage, sample_times in zip([\"W\", \"N1\", \"N2\", \"N3\", \"R\"], [sample_W, sample_N1, sample_N1, sample_N2, sample_N3, sample_R]):\n",
    "            for j, t in enumerate(sample_times):\n",
    "                start = int((t - half_window_sec) * fs)\n",
    "                end = int((t + half_window_sec) * fs)\n",
    "                if start < 0 or end > total_samples:\n",
    "                    continue\n",
    "                win = signals[i][start:end]\n",
    "                win = notch_filter(win, fs, freq=60, Q=30)\n",
    "                win = lowpass_filter(win, fs, cutoff=90, order=4)\n",
    "                ds_win = downsample_window(win, fs, target_fs=200)\n",
    "                key = f\"{subject.lower()}_{stage.lower()}_win_{j+1}_{ch_label.replace('-', '').replace(' ', '')}\"\n",
    "                # Save both the processed window and the center time index.\n",
    "                subject_windows[key] = {\"window\": ds_win, \"time_index\": t}\n",
    "\n",
    "    out_path = os.path.join(subj_path, \"complete_extracted_windows_group7.pkl\")\n",
    "    with open(out_path, \"wb\") as f:\n",
    "        pickle.dump(subject_windows, f)\n",
    "    print(f\"Saved {len(subject_windows)} windows for {subject} (electrode group 7) at {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BATCH 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m total_samples \u001b[38;5;241m=\u001b[39m reader\u001b[38;5;241m.\u001b[39mgetNSamples()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     45\u001b[0m n_channels \u001b[38;5;241m=\u001b[39m reader\u001b[38;5;241m.\u001b[39msignals_in_file\n\u001b[0;32m---> 46\u001b[0m signals \u001b[38;5;241m=\u001b[39m [\u001b[43mreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadSignal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_channels)]\n\u001b[1;32m     47\u001b[0m channel_labels \u001b[38;5;241m=\u001b[39m reader\u001b[38;5;241m.\u001b[39mgetSignalLabels()\n\u001b[1;32m     48\u001b[0m reader\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pyedflib/edfreader.py:825\u001b[0m, in \u001b[0;36mEdfReader.readSignal\u001b[0;34m(self, chn, start, n, digital)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_digital_signal(chn, start, n, x)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadsignal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    826\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "subject_dirs = sorted([d for d in os.listdir(data_dir)\n",
    "                       if d.lower().startswith(\"epctl\") and os.path.isdir(os.path.join(data_dir, d))])\n",
    "\n",
    "# Split 93 electrodes into 10 groups and select the first group.\n",
    "electrode_indices = np.arange(93)\n",
    "electrode_groups = np.array_split(electrode_indices, 10)\n",
    "# first_group = electrode_groups[0]\n",
    "group = electrode_groups[7]\n",
    "\n",
    "# subject_batch = subject_dirs[21:22] # done\n",
    "# subject_batch = subject_dirs[22:23] # done\n",
    "subject_batch = subject_dirs[23:24] # done\n",
    "\n",
    "k = 2  # windows per electrode per subject (so k/2 for \"W\" and k/2 for \"N1\")\n",
    "n_per_stage = k // 2\n",
    "window_sec = 2   # window spans 2 sec (±1 sec)\n",
    "half_window_sec = window_sec / 2\n",
    "\n",
    "for subject in subject_batch:\n",
    "    subj_path = os.path.join(data_dir, subject)\n",
    "    csv_files = glob.glob(os.path.join(subj_path, \"*.csv\"))\n",
    "    edf_files = glob.glob(os.path.join(subj_path, \"*.edf\"))\n",
    "    if not csv_files or not edf_files:\n",
    "        print(f\"Skipping {subject}: missing CSV or EDF.\")\n",
    "        continue\n",
    "    df = pd.read_csv(csv_files[0], index_col=0)\n",
    "    df_W = df[df[\"stage\"]==\"W\"]\n",
    "    df_N1 = df[df[\"stage\"]==\"N1\"]\n",
    "    df_N2 = df[df[\"stage\"]==\"N2\"]\n",
    "    df_N3 = df[df[\"stage\"]==\"N3\"]\n",
    "    df_R = df[df[\"stage\"]==\"R\"]\n",
    "    if len(df_W) < n_per_stage or len(df_N1) < n_per_stage:\n",
    "        print(f\"Skipping {subject}: not enough epochs for one stage.\")\n",
    "        continue\n",
    "    # fixed random_state\n",
    "    sample_W = df_W.sample(n=n_per_stage, random_state=0)[\"time_index\"].values\n",
    "    sample_N1 = df_N1.sample(n=n_per_stage, random_state=0)[\"time_index\"].values\n",
    "    sample_N2 = df_N2.sample(n=n_per_stage, random_state=0)[\"time_index\"].values\n",
    "    sample_N3 = df_N3.sample(n=n_per_stage, random_state=0)[\"time_index\"].values\n",
    "    sample_R = df_R.sample(n=n_per_stage, random_state=0)[\"time_index\"].values\n",
    "\n",
    "    reader = pyedflib.EdfReader(edf_files[0])\n",
    "    fs = reader.getSampleFrequency(0)  # 1000 Hz\n",
    "    total_samples = reader.getNSamples()[0]\n",
    "    n_channels = reader.signals_in_file\n",
    "    signals = [reader.readSignal(i) for i in range(n_channels)]\n",
    "    channel_labels = reader.getSignalLabels()\n",
    "    reader.close()\n",
    "\n",
    "    subject_windows = {}\n",
    "    for i in group:\n",
    "        if i >= len(channel_labels):\n",
    "            continue\n",
    "        ch_label = channel_labels[i]\n",
    "        for stage, sample_times in zip([\"W\", \"N1\", \"N2\", \"N3\", \"R\"], [sample_W, sample_N1, sample_N1, sample_N2, sample_N3, sample_R]):\n",
    "            for j, t in enumerate(sample_times):\n",
    "                start = int((t - half_window_sec) * fs)\n",
    "                end = int((t + half_window_sec) * fs)\n",
    "                if start < 0 or end > total_samples:\n",
    "                    continue\n",
    "                win = signals[i][start:end]\n",
    "                win = notch_filter(win, fs, freq=60, Q=30)\n",
    "                win = lowpass_filter(win, fs, cutoff=90, order=4)\n",
    "                ds_win = downsample_window(win, fs, target_fs=200)\n",
    "                key = f\"{subject.lower()}_{stage.lower()}_win_{j+1}_{ch_label.replace('-', '').replace(' ', '')}\"\n",
    "                # Save both the processed window and the center time index.\n",
    "                subject_windows[key] = {\"window\": ds_win, \"time_index\": t}\n",
    "\n",
    "    out_path = os.path.join(subj_path, \"complete_extracted_windows_group8.pkl\")\n",
    "    with open(out_path, \"wb\") as f:\n",
    "        pickle.dump(subject_windows, f)\n",
    "    print(f\"Saved {len(subject_windows)} windows for {subject} (electrode group 8) at {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BATCH 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_dirs = sorted([d for d in os.listdir(data_dir)\n",
    "                       if d.lower().startswith(\"epctl\") and os.path.isdir(os.path.join(data_dir, d))])\n",
    "\n",
    "# Split 93 electrodes into 10 groups and select the first group.\n",
    "electrode_indices = np.arange(93)\n",
    "electrode_groups = np.array_split(electrode_indices, 10)\n",
    "group = electrode_groups[8]\n",
    "\n",
    "# subject_batch = subject_dirs[24:25] # done\n",
    "# subject_batch = subject_dirs[25:26] # done\n",
    "subject_batch = subject_dirs[26:27] # done\n",
    "\n",
    "k = 2  # windows per electrode per subject (so k/2 for \"W\" and k/2 for \"N1\")\n",
    "n_per_stage = k // 2\n",
    "window_sec = 2   # window spans 2 sec (±1 sec)\n",
    "half_window_sec = window_sec / 2\n",
    "\n",
    "for subject in subject_batch:\n",
    "    subj_path = os.path.join(data_dir, subject)\n",
    "    csv_files = glob.glob(os.path.join(subj_path, \"*.csv\"))\n",
    "    edf_files = glob.glob(os.path.join(subj_path, \"*.edf\"))\n",
    "    if not csv_files or not edf_files:\n",
    "        print(f\"Skipping {subject}: missing CSV or EDF.\")\n",
    "        continue\n",
    "    df = pd.read_csv(csv_files[0], index_col=0)\n",
    "    df_W = df[df[\"stage\"]==\"W\"]\n",
    "    df_N1 = df[df[\"stage\"]==\"N1\"]\n",
    "    df_N2 = df[df[\"stage\"]==\"N2\"]\n",
    "    df_N3 = df[df[\"stage\"]==\"N3\"]\n",
    "    df_R = df[df[\"stage\"]==\"R\"]\n",
    "    if len(df_W) < n_per_stage or len(df_N1) < n_per_stage:\n",
    "        print(f\"Skipping {subject}: not enough epochs for one stage.\")\n",
    "        continue\n",
    "    # fixed random_state\n",
    "    sample_W = df_W.sample(n=n_per_stage, random_state=0)[\"time_index\"].values\n",
    "    sample_N1 = df_N1.sample(n=n_per_stage, random_state=0)[\"time_index\"].values\n",
    "    sample_N2 = df_N2.sample(n=n_per_stage, random_state=0)[\"time_index\"].values\n",
    "    sample_N3 = df_N3.sample(n=n_per_stage, random_state=0)[\"time_index\"].values\n",
    "    sample_R = df_R.sample(n=n_per_stage, random_state=0)[\"time_index\"].values\n",
    "\n",
    "\n",
    "    reader = pyedflib.EdfReader(edf_files[0])\n",
    "    fs = reader.getSampleFrequency(0)  # 1000 Hz\n",
    "    total_samples = reader.getNSamples()[0]\n",
    "    n_channels = reader.signals_in_file\n",
    "    signals = [reader.readSignal(i) for i in range(n_channels)]\n",
    "    channel_labels = reader.getSignalLabels()\n",
    "    reader.close()\n",
    "\n",
    "    subject_windows = {}\n",
    "    for i in group:\n",
    "        if i >= len(channel_labels):\n",
    "            continue\n",
    "        ch_label = channel_labels[i]\n",
    "        for stage, sample_times in zip([\"W\", \"N1\", \"N2\", \"N3\", \"R\"], [sample_W, sample_N1, sample_N1, sample_N2, sample_N3, sample_R]):\n",
    "            for j, t in enumerate(sample_times):\n",
    "                start = int((t - half_window_sec) * fs)\n",
    "                end = int((t + half_window_sec) * fs)\n",
    "                if start < 0 or end > total_samples:\n",
    "                    continue\n",
    "                win = signals[i][start:end]\n",
    "                win = notch_filter(win, fs, freq=60, Q=30)\n",
    "                win = lowpass_filter(win, fs, cutoff=90, order=4)\n",
    "                ds_win = downsample_window(win, fs, target_fs=200)\n",
    "                key = f\"{subject.lower()}_{stage.lower()}_win_{j+1}_{ch_label.replace('-', '').replace(' ', '')}\"\n",
    "                # Save both the processed window and the center time index.\n",
    "                subject_windows[key] = {\"window\": ds_win, \"time_index\": t}\n",
    "\n",
    "    out_path = os.path.join(subj_path, \"complete_extracted_windows_group9.pkl\")\n",
    "    with open(out_path, \"wb\") as f:\n",
    "        pickle.dump(subject_windows, f)\n",
    "    print(f\"Saved {len(subject_windows)} windows for {subject} (electrode group 9) at {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BATCH 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'stage'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'stage'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     25\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(csv_files[\u001b[38;5;241m0\u001b[39m], index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m df_W \u001b[38;5;241m=\u001b[39m df[\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     27\u001b[0m df_N1 \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN1\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(df_W) \u001b[38;5;241m<\u001b[39m n_per_stage \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(df_N1) \u001b[38;5;241m<\u001b[39m n_per_stage:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'stage'"
     ]
    }
   ],
   "source": [
    "data_dir = DataConfig.BASE_PATH\n",
    "subject_dirs = sorted([d for d in os.listdir(data_dir)\n",
    "                       if d.lower().startswith(\"epctl\") and os.path.isdir(os.path.join(data_dir, d))])\n",
    "\n",
    "# Split 93 electrodes into 10 groups and select the first group.\n",
    "electrode_indices = np.arange(93)\n",
    "electrode_groups = np.array_split(electrode_indices, 10)\n",
    "group = electrode_groups[9]\n",
    "\n",
    "# subject_batch = subject_dirs[27:28] \n",
    "subject_batch = subject_dirs[28:29] # last one!\n",
    "\n",
    "k = 2  # windows per electrode per subject (so k/2 for \"W\" and k/2 for \"N1\")\n",
    "n_per_stage = k // 2\n",
    "window_sec = 2   # window spans 2 sec (±1 sec)\n",
    "half_window_sec = window_sec / 2\n",
    "\n",
    "for subject in subject_batch:\n",
    "    subj_path = os.path.join(data_dir, subject)\n",
    "    csv_files = glob.glob(os.path.join(subj_path, \"*.csv\"))\n",
    "    edf_files = glob.glob(os.path.join(subj_path, \"*.edf\"))\n",
    "    if not csv_files or not edf_files:\n",
    "        print(f\"Skipping {subject}: missing CSV or EDF.\")\n",
    "        continue\n",
    "    df = pd.read_csv(csv_files[0], index_col=0)\n",
    "    df_W = df[df[\"stage\"]==\"W\"]\n",
    "    df_N1 = df[df[\"stage\"]==\"N1\"]\n",
    "    df_N2 = df[df[\"stage\"]==\"N2\"]\n",
    "    df_N3 = df[df[\"stage\"]==\"N3\"]\n",
    "    df_R = df[df[\"stage\"]==\"R\"]\n",
    "    if len(df_W) < n_per_stage or len(df_N1) < n_per_stage:\n",
    "        print(f\"Skipping {subject}: not enough epochs for one stage.\")\n",
    "        continue\n",
    "    # fixed random_state\n",
    "    sample_W = df_W.sample(n=n_per_stage, random_state=0)[\"time_index\"].values\n",
    "    sample_N1 = df_N1.sample(n=n_per_stage, random_state=0)[\"time_index\"].values\n",
    "    sample_N2 = df_N2.sample(n=n_per_stage, random_state=0)[\"time_index\"].values\n",
    "    sample_N3 = df_N3.sample(n=n_per_stage, random_state=0)[\"time_index\"].values\n",
    "    sample_R = df_R.sample(n=n_per_stage, random_state=0)[\"time_index\"].values\n",
    "\n",
    "    reader = pyedflib.EdfReader(edf_files[0])\n",
    "    fs = reader.getSampleFrequency(0)  # 1000 Hz\n",
    "    total_samples = reader.getNSamples()[0]\n",
    "    n_channels = reader.signals_in_file\n",
    "    signals = [reader.readSignal(i) for i in range(n_channels)]\n",
    "    channel_labels = reader.getSignalLabels()\n",
    "    reader.close()\n",
    "\n",
    "    subject_windows = {}\n",
    "    for i in group:\n",
    "        if i >= len(channel_labels):\n",
    "            continue\n",
    "        ch_label = channel_labels[i]\n",
    "        for stage, sample_times in zip([\"W\", \"N1\", \"N2\", \"N3\", \"R\"], [sample_W, sample_N1, sample_N1, sample_N2, sample_N3, sample_R]):\n",
    "            for j, t in enumerate(sample_times):\n",
    "                start = int((t - half_window_sec) * fs)\n",
    "                end = int((t + half_window_sec) * fs)\n",
    "                if start < 0 or end > total_samples:\n",
    "                    continue\n",
    "                win = signals[i][start:end]\n",
    "                win = notch_filter(win, fs, freq=60, Q=30)\n",
    "                win = lowpass_filter(win, fs, cutoff=90, order=4)\n",
    "                ds_win = downsample_window(win, fs, target_fs=200)\n",
    "                key = f\"{subject.lower()}_{stage.lower()}_win_{j+1}_{ch_label.replace('-', '').replace(' ', '')}\"\n",
    "                # Save both the processed window and the center time index.\n",
    "                subject_windows[key] = {\"window\": ds_win, \"time_index\": t}\n",
    "\n",
    "    out_path = os.path.join(subj_path, \"complete_extracted_windows_group10.pkl\")\n",
    "    with open(out_path, \"wb\") as f:\n",
    "        pickle.dump(subject_windows, f)\n",
    "    print(f\"Saved {len(subject_windows)} windows for {subject} (electrode group 10) at {out_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnt_gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
