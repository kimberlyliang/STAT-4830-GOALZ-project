{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os, glob, numpy as np, mne, pandas as pd\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    " # Example using absolute path (adjust to your actual path)\n",
    "PROJECT_ROOT = \"/Users/kimberly/Documents/STAT4830/STAT-4830-GOALZ-project\"\n",
    "BASE_DIR = os.path.join(PROJECT_ROOT, \"data/sleep-edf-database-expanded-1.0.0\")\n",
    "OUTPUT_DIR = os.path.join(PROJECT_ROOT, 'psd_sleep_edf')\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# USE_MULTIPLE_CHANNELS = True\n",
    "# if USE_MULTIPLE_CHANNELS:\n",
    "#     CHANNELS_TO_LOAD = [\"EEG Fpz-Cz\", \"EOG horizontal\"]\n",
    "# else:\n",
    "#     CHANNELS_TO_LOAD = [\"EEG Fpz-Cz\"]\n",
    "\n",
    "TARGET_SFREQ = 100.0\n",
    "LOW_FREQ = 0.5\n",
    "HIGH_FREQ = 30.0\n",
    "EPOCH_LENGTH = 30.0\n",
    "SEQ_LENGTH = 20\n",
    "SEQ_STRIDE = 10\n",
    "\n",
    "ANNOTATION_MAP = {\n",
    "    \"Sleep stage W\": 0,\n",
    "    \"Sleep stage 1\": 1,\n",
    "    \"Sleep stage 2\": 2,\n",
    "    \"Sleep stage 3\": 3,\n",
    "    \"Sleep stage 4\": 3,\n",
    "    \"Sleep stage R\": 4\n",
    "}\n",
    "\n",
    "def process_record(psg_path, hyp_path, channels, target_sfreq, low_freq, high_freq, epoch_length):\n",
    "    raw = mne.io.read_raw_edf(psg_path, include=channels, preload=True, verbose=False)\n",
    "    if raw.info['sfreq'] != target_sfreq:\n",
    "        raw.resample(target_sfreq, npad=\"auto\", verbose=False)\n",
    "    picks = mne.pick_types(raw.info, eeg=True, eog=True)\n",
    "    raw.filter(l_freq=low_freq, h_freq=high_freq, picks=picks, verbose=False)\n",
    "    ann = mne.read_annotations(hyp_path)\n",
    "    raw.set_annotations(ann, emit_warning=False)\n",
    "    events, _ = mne.events_from_annotations(raw, event_id=ANNOTATION_MAP, chunk_duration=epoch_length)\n",
    "    tmin = 0.0; tmax = epoch_length - 1/raw.info['sfreq']\n",
    "    epochs = mne.Epochs(raw, events=events, event_id=ANNOTATION_MAP, tmin=tmin, tmax=tmax,\n",
    "                        baseline=None, preload=True, verbose=False)\n",
    "    data = epochs.get_data()\n",
    "    labels = epochs.events[:, -1]\n",
    "    for ch in range(data.shape[1]):\n",
    "        m = np.mean(data[:, ch, :])\n",
    "        s = np.std(data[:, ch, :]) if np.std(data[:, ch, :]) != 0 else 1.0\n",
    "        data[:, ch, :] = (data[:, ch, :] - m) / s\n",
    "    return data, labels, raw.ch_names\n",
    "\n",
    "def create_sequences(data, labels, seq_length, seq_stride):\n",
    "    n_epochs = data.shape[0]\n",
    "    sequences, seq_labels = [], []\n",
    "    for start in range(0, n_epochs - seq_length + 1, seq_stride):\n",
    "        sequences.append(data[start:start+seq_length])\n",
    "        seq_labels.append(labels[start:start+seq_length])\n",
    "    return np.array(sequences), np.array(seq_labels)\n",
    "\n",
    "def find_hypnogram(psg_file):\n",
    "    # Extract subject ID (assumes first 6 characters, e.g., \"SC4001\")\n",
    "    subject_id = os.path.basename(psg_file)[:6]\n",
    "    dir_path = os.path.dirname(psg_file)\n",
    "    pattern = os.path.join(dir_path, f\"{subject_id}*Hypnogram.edf\")\n",
    "    hyp_files = glob.glob(pattern)\n",
    "    if len(hyp_files) == 1:\n",
    "        return hyp_files[0]\n",
    "    elif len(hyp_files) > 1:\n",
    "        return hyp_files[0]  # or choose based on additional rules if needed\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def process_and_save(psg_file, output_dir, channels):\n",
    "    hyp_file = find_hypnogram(psg_file)\n",
    "    if not hyp_file:\n",
    "        print(f\"Hypnogram not found for {psg_file}, skipping.\")\n",
    "        return\n",
    "    try:\n",
    "        data, labels, ch_names = process_record(psg_file, hyp_file, channels,\n",
    "                                                  TARGET_SFREQ, LOW_FREQ, HIGH_FREQ, EPOCH_LENGTH)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {psg_file}: {e}\")\n",
    "        return\n",
    "    rec_id = os.path.basename(psg_file).replace('-PSG.edf', '')\n",
    "    np.savez_compressed(os.path.join(output_dir, f\"{rec_id}_epochs.npz\"),\n",
    "                        data=data.astype('float32'), labels=labels.astype('int8'))\n",
    "    sequences, seq_labels = create_sequences(data, labels, SEQ_LENGTH, SEQ_STRIDE)\n",
    "    np.savez_compressed(os.path.join(output_dir, f\"{rec_id}_sequences.npz\"),\n",
    "                        sequences=sequences.astype('float32'), seq_labels=seq_labels.astype('int8'))\n",
    "    print(f\"Processed {rec_id}: epochs {data.shape[0]}, sequences {sequences.shape[0]}, channels: {ch_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 197 PSG files.\n",
      "Used Annotations descriptions: [np.str_('Sleep stage 1'), np.str_('Sleep stage 2'), np.str_('Sleep stage 3'), np.str_('Sleep stage R'), np.str_('Sleep stage W')]\n",
      "Used Annotations descriptions: [np.str_('Sleep stage 1'), np.str_('Sleep stage 2'), np.str_('Sleep stage 3'), np.str_('Sleep stage 4'), np.str_('Sleep stage R'), np.str_('Sleep stage W')]\n",
      "Processed SC4201E0: epochs 2803, sequences 279, channels: ['EEG Fpz-Cz', 'EOG horizontal']\n",
      "Processed SC4171E0: epochs 2741, sequences 273, channels: ['EEG Fpz-Cz', 'EOG horizontal']\n",
      "Used Annotations descriptions: [np.str_('Sleep stage 1'), np.str_('Sleep stage 2'), np.str_('Sleep stage 3'), np.str_('Sleep stage 4'), np.str_('Sleep stage R'), np.str_('Sleep stage W')]\n",
      "Used Annotations descriptions: [np.str_('Sleep stage 1'), np.str_('Sleep stage 2'), np.str_('Sleep stage 3'), np.str_('Sleep stage 4'), np.str_('Sleep stage R'), np.str_('Sleep stage W')]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m     Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)(delayed(process_and_save)(f, OUTPUT_DIR, CHANNELS_TO_LOAD) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m psg_files)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m----> 9\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     psg_files\u001b[38;5;241m.\u001b[39mextend(glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(BASE_DIR, sub, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*-PSG.edf\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(psg_files)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m PSG files.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_and_save\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOUTPUT_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCHANNELS_TO_LOAD\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpsg_files\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    psg_files = []\n",
    "    for sub in SUBFOLDERS:\n",
    "        psg_files.extend(glob.glob(os.path.join(BASE_DIR, sub, '*-PSG.edf')))\n",
    "    print(f\"Found {len(psg_files)} PSG files.\")\n",
    "    Parallel(n_jobs=2)(delayed(process_and_save)(f, OUTPUT_DIR, CHANNELS_TO_LOAD) for f in psg_files)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../data/psd_sleep_edf/ST7121J0_epochs.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load epoch-level data\u001b[39;00m\n\u001b[1;32m      5\u001b[0m epoch_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(OUTPUT_DIR, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrec_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_epochs.npz\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m epoch_data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m data \u001b[38;5;241m=\u001b[39m epoch_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m]      \u001b[38;5;66;03m# shape: (n_epochs, n_channels, n_samples)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m labels \u001b[38;5;241m=\u001b[39m epoch_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# shape: (n_epochs,)\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/numpy/lib/_npyio_impl.py:451\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    449\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 451\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    452\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../data/psd_sleep_edf/ST7121J0_epochs.npz'"
     ]
    }
   ],
   "source": [
    "OUTPUT_DIR = '../../data/psd_sleep_edf'\n",
    "rec_id = 'ST7121J0'\n",
    "\n",
    "# Load epoch-level data\n",
    "epoch_file = os.path.join(OUTPUT_DIR, f\"{rec_id}_epochs.npz\")\n",
    "epoch_data = np.load(epoch_file)\n",
    "data = epoch_data['data']      # shape: (n_epochs, n_channels, n_samples)\n",
    "labels = epoch_data['labels']  # shape: (n_epochs,)\n",
    "\n",
    "print(f\"Epoch-level data shape: {data.shape}\")\n",
    "print(f\"Epoch-level labels shape: {labels.shape}\")\n",
    "print(\"First 10 labels:\", labels[:100])\n",
    "\n",
    "# Load sequence-level data\n",
    "seq_file = os.path.join(OUTPUT_DIR, f\"{rec_id}_sequences.npz\")\n",
    "seq_data = np.load(seq_file)\n",
    "sequences = seq_data['sequences']       # shape: (n_sequences, SEQ_LENGTH, n_channels, n_samples)\n",
    "seq_labels = seq_data['seq_labels']     # shape: (n_sequences, SEQ_LENGTH)\n",
    "\n",
    "print(f\"Sequence-level data shape: {sequences.shape}\")\n",
    "print(f\"Sequence-level labels shape: {seq_labels.shape}\")\n",
    "print(\"First sequence labels:\", seq_labels[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotation descriptions: ['Sleep stage W' 'Sleep stage 1' 'Sleep stage 2' 'Sleep stage 1'\n",
      " 'Sleep stage 2' 'Sleep stage 3' 'Sleep stage 4' 'Sleep stage 3'\n",
      " 'Sleep stage 4' 'Sleep stage 3' 'Sleep stage 4' 'Sleep stage 3'\n",
      " 'Sleep stage 2' 'Sleep stage R' 'Sleep stage 1' 'Sleep stage 2'\n",
      " 'Sleep stage 1' 'Sleep stage 2' 'Sleep stage 3' 'Sleep stage 2'\n",
      " 'Sleep stage 3' 'Sleep stage 4' 'Sleep stage 3' 'Sleep stage 2'\n",
      " 'Sleep stage 1' 'Sleep stage 2' 'Sleep stage W' 'Sleep stage 1'\n",
      " 'Sleep stage 2' 'Sleep stage W' 'Sleep stage 1' 'Sleep stage 2'\n",
      " 'Sleep stage W' 'Sleep stage 1' 'Sleep stage 2' 'Sleep stage R'\n",
      " 'Sleep stage 2' 'Sleep stage 3' 'Sleep stage 2' 'Sleep stage 3'\n",
      " 'Sleep stage 2' 'Sleep stage 3' 'Sleep stage 2' 'Sleep stage 3'\n",
      " 'Sleep stage 2' 'Sleep stage 1' 'Sleep stage 2' 'Sleep stage 3'\n",
      " 'Sleep stage 2' 'Sleep stage 3' 'Sleep stage 2' 'Sleep stage 3'\n",
      " 'Sleep stage 4' 'Sleep stage 3' 'Sleep stage 4' 'Sleep stage 3'\n",
      " 'Sleep stage 4' 'Sleep stage 3' 'Sleep stage 4' 'Sleep stage 3'\n",
      " 'Sleep stage 4' 'Sleep stage W' 'Sleep stage 1' 'Sleep stage 2'\n",
      " 'Sleep stage 1' 'Sleep stage 2' 'Sleep stage 1' 'Sleep stage 2'\n",
      " 'Sleep stage R' 'Sleep stage 2' 'Sleep stage W' 'Sleep stage 1'\n",
      " 'Sleep stage W' 'Sleep stage 1' 'Sleep stage W' 'Sleep stage 1'\n",
      " 'Sleep stage 2' 'Sleep stage 1' 'Sleep stage 2' 'Sleep stage 3'\n",
      " 'Sleep stage 2' 'Sleep stage 3' 'Sleep stage W' 'Sleep stage 1'\n",
      " 'Sleep stage W' 'Sleep stage 1' 'Sleep stage 2' 'Sleep stage 1'\n",
      " 'Sleep stage 2' 'Sleep stage R' 'Sleep stage 2' 'Sleep stage W'\n",
      " 'Sleep stage 1' 'Sleep stage 2' 'Sleep stage 1' 'Sleep stage 2'\n",
      " 'Sleep stage 1' 'Sleep stage 2' 'Sleep stage 1' 'Sleep stage 2'\n",
      " 'Sleep stage 1' 'Sleep stage 2' 'Sleep stage W' 'Sleep stage 1'\n",
      " 'Sleep stage W' 'Sleep stage 1' 'Sleep stage 2' 'Sleep stage 1'\n",
      " 'Sleep stage 2' 'Sleep stage W' 'Sleep stage 1' 'Sleep stage R'\n",
      " 'Sleep stage W' 'Sleep stage R' 'Sleep stage 1' 'Sleep stage 2'\n",
      " 'Sleep stage 1' 'Sleep stage W' 'Sleep stage 1' 'Sleep stage 2'\n",
      " 'Sleep stage 1' 'Sleep stage W' 'Sleep stage 1' 'Sleep stage R'\n",
      " 'Sleep stage W' 'Sleep stage R' 'Sleep stage W' 'Sleep stage ?'\n",
      " 'Sleep stage ?' 'Sleep stage ?' 'Sleep stage ?']\n"
     ]
    }
   ],
   "source": [
    "hyp_path = '/Users/tereza/spring_2025/STAT_4830/STAT-4830-GOALZ-project/data/sleep-edf-database-expanded-1.0.0/sleep-cassette/SC4121EC-Hypnogram.edf'\n",
    "ann = mne.read_annotations(hyp_path)\n",
    "print(\"Annotation descriptions:\", ann.description)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
